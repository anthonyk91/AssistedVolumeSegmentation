diff -ruN medicaldetectiontoolkit/default_configs.py revised/default_configs.py
--- medicaldetectiontoolkit/default_configs.py	2021-06-27 22:37:22.710564669 +0100
+++ revised/default_configs.py	2021-06-24 21:53:56.158213420 +0100
@@ -137,4 +137,6 @@
         # for probabilistic detection
         self.n_latent_dims = 0
 
-
+        # choose between segmentation methods for returning segmentation output.  None for no output, "global" for a
+        # single global segmentation and "instance" for instance segmentation
+        self.segmentation_method = None
diff -ruN medicaldetectiontoolkit/exec.py revised/exec.py
--- medicaldetectiontoolkit/exec.py	2021-06-27 22:37:22.710564669 +0100
+++ revised/exec.py	2021-06-20 23:35:17.403058480 +0100
@@ -86,13 +86,19 @@
             net.eval()
             if cf.do_validation:
                 val_results_list = []
-                val_predictor = Predictor(cf, net, logger, mode='val')
-                for _ in range(batch_gen['n_val']):
+                if cf.val_mode == 'val_patient':
+                    val_predictor = Predictor(cf, net, logger, mode='val')
+                    num_val = batch_gen['n_val']
+                elif cf.val_mode == 'val_sampling':
+                    num_val = cf.num_val_batches
+                for vix in range(num_val):
                     batch = next(batch_gen[cf.val_mode])
                     if cf.val_mode == 'val_patient':
                         results_dict = val_predictor.predict_patient(batch)
                     elif cf.val_mode == 'val_sampling':
                         results_dict = net.train_forward(batch, is_validation=True)
+                    logger.info('val. batch {0}/{1} (ep. {2}) ||'
+                                .format(vix + 1, num_val, epoch) + results_dict['logger_string'])
                     val_results_list.append([results_dict['boxes'], batch['pid']])
                     monitor_metrics['val']['monitor_values'][epoch].append(results_dict['monitor_values'])
 
diff -ruN medicaldetectiontoolkit/experiments/lidc_exp/configs.py revised/experiments/lidc_exp/configs.py
--- medicaldetectiontoolkit/experiments/lidc_exp/configs.py	2021-06-27 22:37:22.710564669 +0100
+++ revised/experiments/lidc_exp/configs.py	2020-10-18 22:08:02.220959710 +0100
@@ -52,7 +52,8 @@
         # path to preprocessed data.
         self.pp_name = 'lidc_mdt'
         self.input_df_name = 'info_df.pickle'
-        self.pp_data_path = '/media/gregor/HDD2TB/data/lidc/{}'.format(self.pp_name)
+        self.pp_data_path = '/tmp/{}'.format(self.pp_name)
+        #self.pp_data_path = '/media/gregor/HDD2TB/data/lidc/{}'.format(self.pp_name)
         self.pp_test_data_path = self.pp_data_path #change if test_data in separate folder.
 
         # settings for deployment in cloud.
@@ -331,4 +332,4 @@
             self.num_seg_classes = 3 if self.class_specific_seg_flag else 2
 
             if self.model == 'retina_unet':
-                self.operate_stride1 = True
\ No newline at end of file
+                self.operate_stride1 = True
diff -ruN medicaldetectiontoolkit/experiments/toy_exp/configs.py revised/experiments/toy_exp/configs.py
--- medicaldetectiontoolkit/experiments/toy_exp/configs.py	2021-06-27 22:37:22.710564669 +0100
+++ revised/experiments/toy_exp/configs.py	2020-10-18 22:19:39.761997919 +0100
@@ -28,7 +28,8 @@
         #    Preprocessing      #
         #########################
 
-        self.root_dir = '/home/gregor/datasets/toy_mdt'
+        #self.root_dir = '/home/gregor/datasets/toy_mdt'
+        self.root_dir = '/tmp'
 
         #########################
         #         I/O           #
@@ -51,7 +52,8 @@
 
         # choose one of the 3 toy experiments described in https://arxiv.org/pdf/1811.08661.pdf
         # one of ['donuts_shape', 'donuts_pattern', 'circles_scale'].
-        toy_mode = 'donuts_shape_noise'
+        #toy_mode = 'donuts_shape_noise'
+        toy_mode = 'donuts_shape'
 
 
         # path to preprocessed data.
diff -ruN medicaldetectiontoolkit/.git/config revised/.git/config
--- medicaldetectiontoolkit/.git/config	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/config	1970-01-01 01:00:00.000000000 +0100
@@ -1,11 +0,0 @@
-[core]
-	repositoryformatversion = 0
-	filemode = true
-	bare = false
-	logallrefupdates = true
-[remote "origin"]
-	url = https://github.com/MIC-DKFZ/medicaldetectiontoolkit.git
-	fetch = +refs/heads/*:refs/remotes/origin/*
-[branch "master"]
-	remote = origin
-	merge = refs/heads/master
diff -ruN medicaldetectiontoolkit/.git/description revised/.git/description
--- medicaldetectiontoolkit/.git/description	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/description	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-Unnamed repository; edit this file 'description' to name the repository.
diff -ruN medicaldetectiontoolkit/.git/HEAD revised/.git/HEAD
--- medicaldetectiontoolkit/.git/HEAD	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/HEAD	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-ref: refs/heads/master
diff -ruN medicaldetectiontoolkit/.git/hooks/applypatch-msg.sample revised/.git/hooks/applypatch-msg.sample
--- medicaldetectiontoolkit/.git/hooks/applypatch-msg.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/applypatch-msg.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,15 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message taken by
-# applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.  The hook is
-# allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "applypatch-msg".
-
-. git-sh-setup
-commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
-test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
-:
diff -ruN medicaldetectiontoolkit/.git/hooks/commit-msg.sample revised/.git/hooks/commit-msg.sample
--- medicaldetectiontoolkit/.git/hooks/commit-msg.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/commit-msg.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,24 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to check the commit log message.
-# Called by "git commit" with one argument, the name of the file
-# that has the commit message.  The hook should exit with non-zero
-# status after issuing an appropriate message if it wants to stop the
-# commit.  The hook is allowed to edit the commit message file.
-#
-# To enable this hook, rename this file to "commit-msg".
-
-# Uncomment the below to add a Signed-off-by line to the message.
-# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
-# hook is more suited to it.
-#
-# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"
-
-# This example catches duplicate Signed-off-by lines.
-
-test "" = "$(grep '^Signed-off-by: ' "$1" |
-	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
-	echo >&2 Duplicate Signed-off-by lines.
-	exit 1
-}
diff -ruN medicaldetectiontoolkit/.git/hooks/fsmonitor-watchman.sample revised/.git/hooks/fsmonitor-watchman.sample
--- medicaldetectiontoolkit/.git/hooks/fsmonitor-watchman.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/fsmonitor-watchman.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,173 +0,0 @@
-#!/usr/bin/perl
-
-use strict;
-use warnings;
-use IPC::Open2;
-
-# An example hook script to integrate Watchman
-# (https://facebook.github.io/watchman/) with git to speed up detecting
-# new and modified files.
-#
-# The hook is passed a version (currently 2) and last update token
-# formatted as a string and outputs to stdout a new update token and
-# all files that have been modified since the update token. Paths must
-# be relative to the root of the working tree and separated by a single NUL.
-#
-# To enable this hook, rename this file to "query-watchman" and set
-# 'git config core.fsmonitor .git/hooks/query-watchman'
-#
-my ($version, $last_update_token) = @ARGV;
-
-# Uncomment for debugging
-# print STDERR "$0 $version $last_update_token\n";
-
-# Check the hook interface version
-if ($version ne 2) {
-	die "Unsupported query-fsmonitor hook version '$version'.\n" .
-	    "Falling back to scanning...\n";
-}
-
-my $git_work_tree = get_working_dir();
-
-my $retry = 1;
-
-my $json_pkg;
-eval {
-	require JSON::XS;
-	$json_pkg = "JSON::XS";
-	1;
-} or do {
-	require JSON::PP;
-	$json_pkg = "JSON::PP";
-};
-
-launch_watchman();
-
-sub launch_watchman {
-	my $o = watchman_query();
-	if (is_work_tree_watched($o)) {
-		output_result($o->{clock}, @{$o->{files}});
-	}
-}
-
-sub output_result {
-	my ($clockid, @files) = @_;
-
-	# Uncomment for debugging watchman output
-	# open (my $fh, ">", ".git/watchman-output.out");
-	# binmode $fh, ":utf8";
-	# print $fh "$clockid\n@files\n";
-	# close $fh;
-
-	binmode STDOUT, ":utf8";
-	print $clockid;
-	print "\0";
-	local $, = "\0";
-	print @files;
-}
-
-sub watchman_clock {
-	my $response = qx/watchman clock "$git_work_tree"/;
-	die "Failed to get clock id on '$git_work_tree'.\n" .
-		"Falling back to scanning...\n" if $? != 0;
-
-	return $json_pkg->new->utf8->decode($response);
-}
-
-sub watchman_query {
-	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
-	or die "open2() failed: $!\n" .
-	"Falling back to scanning...\n";
-
-	# In the query expression below we're asking for names of files that
-	# changed since $last_update_token but not from the .git folder.
-	#
-	# To accomplish this, we're using the "since" generator to use the
-	# recency index to select candidate nodes and "fields" to limit the
-	# output to file names only. Then we're using the "expression" term to
-	# further constrain the results.
-	if (substr($last_update_token, 0, 1) eq "c") {
-		$last_update_token = "\"$last_update_token\"";
-	}
-	my $query = <<"	END";
-		["query", "$git_work_tree", {
-			"since": $last_update_token,
-			"fields": ["name"],
-			"expression": ["not", ["dirname", ".git"]]
-		}]
-	END
-
-	# Uncomment for debugging the watchman query
-	# open (my $fh, ">", ".git/watchman-query.json");
-	# print $fh $query;
-	# close $fh;
-
-	print CHLD_IN $query;
-	close CHLD_IN;
-	my $response = do {local $/; <CHLD_OUT>};
-
-	# Uncomment for debugging the watch response
-	# open ($fh, ">", ".git/watchman-response.json");
-	# print $fh $response;
-	# close $fh;
-
-	die "Watchman: command returned no output.\n" .
-	"Falling back to scanning...\n" if $response eq "";
-	die "Watchman: command returned invalid output: $response\n" .
-	"Falling back to scanning...\n" unless $response =~ /^\{/;
-
-	return $json_pkg->new->utf8->decode($response);
-}
-
-sub is_work_tree_watched {
-	my ($output) = @_;
-	my $error = $output->{error};
-	if ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {
-		$retry--;
-		my $response = qx/watchman watch "$git_work_tree"/;
-		die "Failed to make watchman watch '$git_work_tree'.\n" .
-		    "Falling back to scanning...\n" if $? != 0;
-		$output = $json_pkg->new->utf8->decode($response);
-		$error = $output->{error};
-		die "Watchman: $error.\n" .
-		"Falling back to scanning...\n" if $error;
-
-		# Uncomment for debugging watchman output
-		# open (my $fh, ">", ".git/watchman-output.out");
-		# close $fh;
-
-		# Watchman will always return all files on the first query so
-		# return the fast "everything is dirty" flag to git and do the
-		# Watchman query just to get it over with now so we won't pay
-		# the cost in git to look up each individual file.
-		my $o = watchman_clock();
-		$error = $output->{error};
-
-		die "Watchman: $error.\n" .
-		"Falling back to scanning...\n" if $error;
-
-		output_result($o->{clock}, ("/"));
-		$last_update_token = $o->{clock};
-
-		eval { launch_watchman() };
-		return 0;
-	}
-
-	die "Watchman: $error.\n" .
-	"Falling back to scanning...\n" if $error;
-
-	return 1;
-}
-
-sub get_working_dir {
-	my $working_dir;
-	if ($^O =~ 'msys' || $^O =~ 'cygwin') {
-		$working_dir = Win32::GetCwd();
-		$working_dir =~ tr/\\/\//;
-	} else {
-		require Cwd;
-		$working_dir = Cwd::cwd();
-	}
-
-	return $working_dir;
-}
diff -ruN medicaldetectiontoolkit/.git/hooks/post-update.sample revised/.git/hooks/post-update.sample
--- medicaldetectiontoolkit/.git/hooks/post-update.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/post-update.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare a packed repository for use over
-# dumb transports.
-#
-# To enable this hook, rename this file to "post-update".
-
-exec git update-server-info
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-applypatch.sample revised/.git/hooks/pre-applypatch.sample
--- medicaldetectiontoolkit/.git/hooks/pre-applypatch.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-applypatch.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,14 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed
-# by applypatch from an e-mail message.
-#
-# The hook should exit with non-zero status after issuing an
-# appropriate message if it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-applypatch".
-
-. git-sh-setup
-precommit="$(git rev-parse --git-path hooks/pre-commit)"
-test -x "$precommit" && exec "$precommit" ${1+"$@"}
-:
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-commit.sample revised/.git/hooks/pre-commit.sample
--- medicaldetectiontoolkit/.git/hooks/pre-commit.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-commit.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,49 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed.
-# Called by "git commit" with no arguments.  The hook should
-# exit with non-zero status after issuing an appropriate message if
-# it wants to stop the commit.
-#
-# To enable this hook, rename this file to "pre-commit".
-
-if git rev-parse --verify HEAD >/dev/null 2>&1
-then
-	against=HEAD
-else
-	# Initial commit: diff against an empty tree object
-	against=$(git hash-object -t tree /dev/null)
-fi
-
-# If you want to allow non-ASCII filenames set this variable to true.
-allownonascii=$(git config --type=bool hooks.allownonascii)
-
-# Redirect output to stderr.
-exec 1>&2
-
-# Cross platform projects tend to avoid non-ASCII filenames; prevent
-# them from being added to the repository. We exploit the fact that the
-# printable range starts at the space character and ends with tilde.
-if [ "$allownonascii" != "true" ] &&
-	# Note that the use of brackets around a tr range is ok here, (it's
-	# even required, for portability to Solaris 10's /usr/bin/tr), since
-	# the square bracket bytes happen to fall in the designated range.
-	test $(git diff --cached --name-only --diff-filter=A -z $against |
-	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
-then
-	cat <<\EOF
-Error: Attempt to add a non-ASCII file name.
-
-This can cause problems if you want to work with people on other platforms.
-
-To be portable it is advisable to rename the file.
-
-If you know what you are doing you can disable this check using:
-
-  git config hooks.allownonascii true
-EOF
-	exit 1
-fi
-
-# If there are whitespace errors, print the offending file names and fail.
-exec git diff-index --check --cached $against --
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-merge-commit.sample revised/.git/hooks/pre-merge-commit.sample
--- medicaldetectiontoolkit/.git/hooks/pre-merge-commit.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-merge-commit.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,13 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to verify what is about to be committed.
-# Called by "git merge" with no arguments.  The hook should
-# exit with non-zero status after issuing an appropriate message to
-# stderr if it wants to stop the merge commit.
-#
-# To enable this hook, rename this file to "pre-merge-commit".
-
-. git-sh-setup
-test -x "$GIT_DIR/hooks/pre-commit" &&
-        exec "$GIT_DIR/hooks/pre-commit"
-:
diff -ruN medicaldetectiontoolkit/.git/hooks/prepare-commit-msg.sample revised/.git/hooks/prepare-commit-msg.sample
--- medicaldetectiontoolkit/.git/hooks/prepare-commit-msg.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/prepare-commit-msg.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,42 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to prepare the commit log message.
-# Called by "git commit" with the name of the file that has the
-# commit message, followed by the description of the commit
-# message's source.  The hook's purpose is to edit the commit
-# message file.  If the hook fails with a non-zero status,
-# the commit is aborted.
-#
-# To enable this hook, rename this file to "prepare-commit-msg".
-
-# This hook includes three examples. The first one removes the
-# "# Please enter the commit message..." help message.
-#
-# The second includes the output of "git diff --name-status -r"
-# into the message, just before the "git status" output.  It is
-# commented because it doesn't cope with --amend or with squashed
-# commits.
-#
-# The third example adds a Signed-off-by line to the message, that can
-# still be edited.  This is rarely a good idea.
-
-COMMIT_MSG_FILE=$1
-COMMIT_SOURCE=$2
-SHA1=$3
-
-/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"
-
-# case "$COMMIT_SOURCE,$SHA1" in
-#  ,|template,)
-#    /usr/bin/perl -i.bak -pe '
-#       print "\n" . `git diff --cached --name-status -r`
-# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
-#  *) ;;
-# esac
-
-# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
-# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
-# if test -z "$COMMIT_SOURCE"
-# then
-#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
-# fi
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-push.sample revised/.git/hooks/pre-push.sample
--- medicaldetectiontoolkit/.git/hooks/pre-push.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-push.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,53 +0,0 @@
-#!/bin/sh
-
-# An example hook script to verify what is about to be pushed.  Called by "git
-# push" after it has checked the remote status, but before anything has been
-# pushed.  If this script exits with a non-zero status nothing will be pushed.
-#
-# This hook is called with the following parameters:
-#
-# $1 -- Name of the remote to which the push is being done
-# $2 -- URL to which the push is being done
-#
-# If pushing without using a named remote those arguments will be equal.
-#
-# Information about the commits which are being pushed is supplied as lines to
-# the standard input in the form:
-#
-#   <local ref> <local oid> <remote ref> <remote oid>
-#
-# This sample shows how to prevent push of commits where the log message starts
-# with "WIP" (work in progress).
-
-remote="$1"
-url="$2"
-
-zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
-
-while read local_ref local_oid remote_ref remote_oid
-do
-	if test "$local_oid" = "$zero"
-	then
-		# Handle delete
-		:
-	else
-		if test "$remote_oid" = "$zero"
-		then
-			# New branch, examine all commits
-			range="$local_oid"
-		else
-			# Update to existing branch, examine new commits
-			range="$remote_oid..$local_oid"
-		fi
-
-		# Check for WIP commit
-		commit=$(git rev-list -n 1 --grep '^WIP' "$range")
-		if test -n "$commit"
-		then
-			echo >&2 "Found WIP commit in $local_ref, not pushing"
-			exit 1
-		fi
-	fi
-done
-
-exit 0
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-rebase.sample revised/.git/hooks/pre-rebase.sample
--- medicaldetectiontoolkit/.git/hooks/pre-rebase.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-rebase.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,169 +0,0 @@
-#!/bin/sh
-#
-# Copyright (c) 2006, 2008 Junio C Hamano
-#
-# The "pre-rebase" hook is run just before "git rebase" starts doing
-# its job, and can prevent the command from running by exiting with
-# non-zero status.
-#
-# The hook is called with the following parameters:
-#
-# $1 -- the upstream the series was forked from.
-# $2 -- the branch being rebased (or empty when rebasing the current branch).
-#
-# This sample shows how to prevent topic branches that are already
-# merged to 'next' branch from getting rebased, because allowing it
-# would result in rebasing already published history.
-
-publish=next
-basebranch="$1"
-if test "$#" = 2
-then
-	topic="refs/heads/$2"
-else
-	topic=`git symbolic-ref HEAD` ||
-	exit 0 ;# we do not interrupt rebasing detached HEAD
-fi
-
-case "$topic" in
-refs/heads/??/*)
-	;;
-*)
-	exit 0 ;# we do not interrupt others.
-	;;
-esac
-
-# Now we are dealing with a topic branch being rebased
-# on top of master.  Is it OK to rebase it?
-
-# Does the topic really exist?
-git show-ref -q "$topic" || {
-	echo >&2 "No such branch $topic"
-	exit 1
-}
-
-# Is topic fully merged to master?
-not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
-if test -z "$not_in_master"
-then
-	echo >&2 "$topic is fully merged to master; better remove it."
-	exit 1 ;# we could allow it, but there is no point.
-fi
-
-# Is topic ever merged to next?  If so you should not be rebasing it.
-only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
-only_next_2=`git rev-list ^master           ${publish} | sort`
-if test "$only_next_1" = "$only_next_2"
-then
-	not_in_topic=`git rev-list "^$topic" master`
-	if test -z "$not_in_topic"
-	then
-		echo >&2 "$topic is already up to date with master"
-		exit 1 ;# we could allow it, but there is no point.
-	else
-		exit 0
-	fi
-else
-	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
-	/usr/bin/perl -e '
-		my $topic = $ARGV[0];
-		my $msg = "* $topic has commits already merged to public branch:\n";
-		my (%not_in_next) = map {
-			/^([0-9a-f]+) /;
-			($1 => 1);
-		} split(/\n/, $ARGV[1]);
-		for my $elem (map {
-				/^([0-9a-f]+) (.*)$/;
-				[$1 => $2];
-			} split(/\n/, $ARGV[2])) {
-			if (!exists $not_in_next{$elem->[0]}) {
-				if ($msg) {
-					print STDERR $msg;
-					undef $msg;
-				}
-				print STDERR " $elem->[1]\n";
-			}
-		}
-	' "$topic" "$not_in_next" "$not_in_master"
-	exit 1
-fi
-
-<<\DOC_END
-
-This sample hook safeguards topic branches that have been
-published from being rewound.
-
-The workflow assumed here is:
-
- * Once a topic branch forks from "master", "master" is never
-   merged into it again (either directly or indirectly).
-
- * Once a topic branch is fully cooked and merged into "master",
-   it is deleted.  If you need to build on top of it to correct
-   earlier mistakes, a new topic branch is created by forking at
-   the tip of the "master".  This is not strictly necessary, but
-   it makes it easier to keep your history simple.
-
- * Whenever you need to test or publish your changes to topic
-   branches, merge them into "next" branch.
-
-The script, being an example, hardcodes the publish branch name
-to be "next", but it is trivial to make it configurable via
-$GIT_DIR/config mechanism.
-
-With this workflow, you would want to know:
-
-(1) ... if a topic branch has ever been merged to "next".  Young
-    topic branches can have stupid mistakes you would rather
-    clean up before publishing, and things that have not been
-    merged into other branches can be easily rebased without
-    affecting other people.  But once it is published, you would
-    not want to rewind it.
-
-(2) ... if a topic branch has been fully merged to "master".
-    Then you can delete it.  More importantly, you should not
-    build on top of it -- other people may already want to
-    change things related to the topic as patches against your
-    "master", so if you need further changes, it is better to
-    fork the topic (perhaps with the same name) afresh from the
-    tip of "master".
-
-Let's look at this example:
-
-		   o---o---o---o---o---o---o---o---o---o "next"
-		  /       /           /           /
-		 /   a---a---b A     /           /
-		/   /               /           /
-	       /   /   c---c---c---c B         /
-	      /   /   /             \         /
-	     /   /   /   b---b C     \       /
-	    /   /   /   /             \     /
-    ---o---o---o---o---o---o---o---o---o---o---o "master"
-
-
-A, B and C are topic branches.
-
- * A has one fix since it was merged up to "next".
-
- * B has finished.  It has been fully merged up to "master" and "next",
-   and is ready to be deleted.
-
- * C has not merged to "next" at all.
-
-We would want to allow C to be rebased, refuse A, and encourage
-B to be deleted.
-
-To compute (1):
-
-	git rev-list ^master ^topic next
-	git rev-list ^master        next
-
-	if these match, topic has not merged in next at all.
-
-To compute (2):
-
-	git rev-list master..topic
-
-	if this is empty, it is fully merged to "master".
-
-DOC_END
diff -ruN medicaldetectiontoolkit/.git/hooks/pre-receive.sample revised/.git/hooks/pre-receive.sample
--- medicaldetectiontoolkit/.git/hooks/pre-receive.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/pre-receive.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,24 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to make use of push options.
-# The example simply echoes all push options that start with 'echoback='
-# and rejects all pushes when the "reject" push option is used.
-#
-# To enable this hook, rename this file to "pre-receive".
-
-if test -n "$GIT_PUSH_OPTION_COUNT"
-then
-	i=0
-	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
-	do
-		eval "value=\$GIT_PUSH_OPTION_$i"
-		case "$value" in
-		echoback=*)
-			echo "echo from the pre-receive-hook: ${value#*=}" >&2
-			;;
-		reject)
-			exit 1
-		esac
-		i=$((i + 1))
-	done
-fi
diff -ruN medicaldetectiontoolkit/.git/hooks/push-to-checkout.sample revised/.git/hooks/push-to-checkout.sample
--- medicaldetectiontoolkit/.git/hooks/push-to-checkout.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/push-to-checkout.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,78 +0,0 @@
-#!/bin/sh
-
-# An example hook script to update a checked-out tree on a git push.
-#
-# This hook is invoked by git-receive-pack(1) when it reacts to git
-# push and updates reference(s) in its repository, and when the push
-# tries to update the branch that is currently checked out and the
-# receive.denyCurrentBranch configuration variable is set to
-# updateInstead.
-#
-# By default, such a push is refused if the working tree and the index
-# of the remote repository has any difference from the currently
-# checked out commit; when both the working tree and the index match
-# the current commit, they are updated to match the newly pushed tip
-# of the branch. This hook is to be used to override the default
-# behaviour; however the code below reimplements the default behaviour
-# as a starting point for convenient modification.
-#
-# The hook receives the commit with which the tip of the current
-# branch is going to be updated:
-commit=$1
-
-# It can exit with a non-zero status to refuse the push (when it does
-# so, it must not modify the index or the working tree).
-die () {
-	echo >&2 "$*"
-	exit 1
-}
-
-# Or it can make any necessary changes to the working tree and to the
-# index to bring them to the desired state when the tip of the current
-# branch is updated to the new commit, and exit with a zero status.
-#
-# For example, the hook can simply run git read-tree -u -m HEAD "$1"
-# in order to emulate git fetch that is run in the reverse direction
-# with git push, as the two-tree form of git read-tree -u -m is
-# essentially the same as git switch or git checkout that switches
-# branches while keeping the local changes in the working tree that do
-# not interfere with the difference between the branches.
-
-# The below is a more-or-less exact translation to shell of the C code
-# for the default behaviour for git's push-to-checkout hook defined in
-# the push_to_deploy() function in builtin/receive-pack.c.
-#
-# Note that the hook will be executed from the repository directory,
-# not from the working tree, so if you want to perform operations on
-# the working tree, you will have to adapt your code accordingly, e.g.
-# by adding "cd .." or using relative paths.
-
-if ! git update-index -q --ignore-submodules --refresh
-then
-	die "Up-to-date check failed"
-fi
-
-if ! git diff-files --quiet --ignore-submodules --
-then
-	die "Working directory has unstaged changes"
-fi
-
-# This is a rough translation of:
-#
-#   head_has_history() ? "HEAD" : EMPTY_TREE_SHA1_HEX
-if git cat-file -e HEAD 2>/dev/null
-then
-	head=HEAD
-else
-	head=$(git hash-object -t tree --stdin </dev/null)
-fi
-
-if ! git diff-index --quiet --cached --ignore-submodules $head --
-then
-	die "Working directory has staged changes"
-fi
-
-if ! git read-tree -u -m "$commit"
-then
-	die "Could not update working tree to new HEAD"
-fi
diff -ruN medicaldetectiontoolkit/.git/hooks/update.sample revised/.git/hooks/update.sample
--- medicaldetectiontoolkit/.git/hooks/update.sample	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/hooks/update.sample	1970-01-01 01:00:00.000000000 +0100
@@ -1,128 +0,0 @@
-#!/bin/sh
-#
-# An example hook script to block unannotated tags from entering.
-# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
-#
-# To enable this hook, rename this file to "update".
-#
-# Config
-# ------
-# hooks.allowunannotated
-#   This boolean sets whether unannotated tags will be allowed into the
-#   repository.  By default they won't be.
-# hooks.allowdeletetag
-#   This boolean sets whether deleting tags will be allowed in the
-#   repository.  By default they won't be.
-# hooks.allowmodifytag
-#   This boolean sets whether a tag may be modified after creation. By default
-#   it won't be.
-# hooks.allowdeletebranch
-#   This boolean sets whether deleting branches will be allowed in the
-#   repository.  By default they won't be.
-# hooks.denycreatebranch
-#   This boolean sets whether remotely creating branches will be denied
-#   in the repository.  By default this is allowed.
-#
-
-# --- Command line
-refname="$1"
-oldrev="$2"
-newrev="$3"
-
-# --- Safety check
-if [ -z "$GIT_DIR" ]; then
-	echo "Don't run this script from the command line." >&2
-	echo " (if you want, you could supply GIT_DIR then run" >&2
-	echo "  $0 <ref> <oldrev> <newrev>)" >&2
-	exit 1
-fi
-
-if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
-	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
-	exit 1
-fi
-
-# --- Config
-allowunannotated=$(git config --type=bool hooks.allowunannotated)
-allowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)
-denycreatebranch=$(git config --type=bool hooks.denycreatebranch)
-allowdeletetag=$(git config --type=bool hooks.allowdeletetag)
-allowmodifytag=$(git config --type=bool hooks.allowmodifytag)
-
-# check for no description
-projectdesc=$(sed -e '1q' "$GIT_DIR/description")
-case "$projectdesc" in
-"Unnamed repository"* | "")
-	echo "*** Project description file hasn't been set" >&2
-	exit 1
-	;;
-esac
-
-# --- Check types
-# if $newrev is 0000...0000, it's a commit to delete a ref.
-zero=$(git hash-object --stdin </dev/null | tr '[0-9a-f]' '0')
-if [ "$newrev" = "$zero" ]; then
-	newrev_type=delete
-else
-	newrev_type=$(git cat-file -t $newrev)
-fi
-
-case "$refname","$newrev_type" in
-	refs/tags/*,commit)
-		# un-annotated tag
-		short_refname=${refname##refs/tags/}
-		if [ "$allowunannotated" != "true" ]; then
-			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
-			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,delete)
-		# delete tag
-		if [ "$allowdeletetag" != "true" ]; then
-			echo "*** Deleting a tag is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/tags/*,tag)
-		# annotated tag
-		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
-		then
-			echo "*** Tag '$refname' already exists." >&2
-			echo "*** Modifying a tag is not allowed in this repository." >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,commit)
-		# branch
-		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
-			echo "*** Creating a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/heads/*,delete)
-		# delete branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	refs/remotes/*,commit)
-		# tracking branch
-		;;
-	refs/remotes/*,delete)
-		# delete tracking branch
-		if [ "$allowdeletebranch" != "true" ]; then
-			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
-			exit 1
-		fi
-		;;
-	*)
-		# Anything else (is there anything else?)
-		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
-		exit 1
-		;;
-esac
-
-# --- Finished
-exit 0
Binary files medicaldetectiontoolkit/.git/index and revised/.git/index differ
diff -ruN medicaldetectiontoolkit/.git/info/exclude revised/.git/info/exclude
--- medicaldetectiontoolkit/.git/info/exclude	2021-06-27 22:37:20.861564739 +0100
+++ revised/.git/info/exclude	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-# git ls-files --others --exclude-from=.git/info/exclude
-# Lines that start with '#' are comments.
-# For a project mostly in C, the following would be a good set of
-# exclude patterns (uncomment them if you want to use them):
-# *.[oa]
-# *~
diff -ruN medicaldetectiontoolkit/.git/logs/HEAD revised/.git/logs/HEAD
--- medicaldetectiontoolkit/.git/logs/HEAD	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/logs/HEAD	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 f678bc5e91c37d7468e69e174b5048c8bb91e6c8 Anthony <neverforever@gmail.com> 1624829842 +0100	clone: from https://github.com/MIC-DKFZ/medicaldetectiontoolkit.git
diff -ruN medicaldetectiontoolkit/.git/logs/refs/heads/master revised/.git/logs/refs/heads/master
--- medicaldetectiontoolkit/.git/logs/refs/heads/master	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/logs/refs/heads/master	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 f678bc5e91c37d7468e69e174b5048c8bb91e6c8 Anthony <neverforever@gmail.com> 1624829842 +0100	clone: from https://github.com/MIC-DKFZ/medicaldetectiontoolkit.git
diff -ruN medicaldetectiontoolkit/.git/logs/refs/remotes/origin/HEAD revised/.git/logs/refs/remotes/origin/HEAD
--- medicaldetectiontoolkit/.git/logs/refs/remotes/origin/HEAD	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/logs/refs/remotes/origin/HEAD	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-0000000000000000000000000000000000000000 f678bc5e91c37d7468e69e174b5048c8bb91e6c8 Anthony <neverforever@gmail.com> 1624829842 +0100	clone: from https://github.com/MIC-DKFZ/medicaldetectiontoolkit.git
Binary files medicaldetectiontoolkit/.git/objects/pack/pack-d5b06d6f0402b74c60efb5c006047db1c6f361ba.idx and revised/.git/objects/pack/pack-d5b06d6f0402b74c60efb5c006047db1c6f361ba.idx differ
Binary files medicaldetectiontoolkit/.git/objects/pack/pack-d5b06d6f0402b74c60efb5c006047db1c6f361ba.pack and revised/.git/objects/pack/pack-d5b06d6f0402b74c60efb5c006047db1c6f361ba.pack differ
diff -ruN medicaldetectiontoolkit/.git/packed-refs revised/.git/packed-refs
--- medicaldetectiontoolkit/.git/packed-refs	2021-06-27 22:37:22.684564670 +0100
+++ revised/.git/packed-refs	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-# pack-refs with: peeled fully-peeled sorted 
-f678bc5e91c37d7468e69e174b5048c8bb91e6c8 refs/remotes/origin/master
-87f279eca3a33920a535f82ae40014fec6e82418 refs/remotes/origin/torch1x
-4d82df9af09390ae3410663e922ae83d09291069 refs/tags/v0.0.1
-^8a2218f89f8bee05af4db95ca8ee991a932fd855
-08f03d302ef6e227c39ee9e71fa82633698c61e2 refs/tags/v0.0.2
-^3b8762847f19423396a958f37e2a937952a78f55
diff -ruN medicaldetectiontoolkit/.git/refs/heads/master revised/.git/refs/heads/master
--- medicaldetectiontoolkit/.git/refs/heads/master	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/refs/heads/master	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-f678bc5e91c37d7468e69e174b5048c8bb91e6c8
diff -ruN medicaldetectiontoolkit/.git/refs/remotes/origin/HEAD revised/.git/refs/remotes/origin/HEAD
--- medicaldetectiontoolkit/.git/refs/remotes/origin/HEAD	2021-06-27 22:37:22.685564670 +0100
+++ revised/.git/refs/remotes/origin/HEAD	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-ref: refs/remotes/origin/master
diff -ruN medicaldetectiontoolkit/models/mrcnn.py revised/models/mrcnn.py
--- medicaldetectiontoolkit/models/mrcnn.py	2021-06-27 22:37:22.711564669 +0100
+++ revised/models/mrcnn.py	2021-06-20 16:08:20.361589525 +0100
@@ -257,6 +257,7 @@
     :param target_class_ids: (n_sampled_rois)
     :return: loss: torch 1D tensor.
     """
+    #print("target class ids", target_class_ids)
     if 0 not in torch.nonzero(target_class_ids > 0).size():
         positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]
         positive_roi_class_ids = target_class_ids[positive_roi_ix].long()
@@ -778,8 +779,12 @@
                     if cf.dim == 2 else mutils.unmold_mask_3D(masks[i], boxes[i], permuted_image_shape))
             # if masks are returned, take max over binary full masks of all predictions in this image.
             # right now only binary masks for plotting/monitoring. for instance segmentation return all proposal maks.
-            final_masks = np.max(np.array(full_masks), 0) if len(full_masks) > 0 else np.zeros(
-                (*permuted_image_shape[:-1],))
+            # final_masks = np.max(np.array(full_masks), 0) if len(full_masks) > 0 else np.zeros(
+            #     (*permuted_image_shape[:-1],))
+            if len(full_masks) > 0:
+                final_masks = np.array(full_masks)
+            else:
+                final_masks = np.zeros((*permuted_image_shape[:-1],))
 
             # add final perdictions to results.
             if 0 not in boxes.shape:
@@ -794,7 +799,8 @@
 
     # create and fill results dictionary.
     results_dict = {'boxes': box_results_list,
-                    'seg_preds': np.round(np.array(seg_preds))[:, np.newaxis].astype('uint8')}
+                    # 'seg_preds': np.round(np.array(seg_preds))[:, np.newaxis].astype('uint8')}
+                    'seg_preds': seg_preds} #np.round(np.array(seg_preds)).astype('uint8')}
 
     return results_dict
 
diff -ruN medicaldetectiontoolkit/models/tmp_backbone.py revised/models/tmp_backbone.py
--- medicaldetectiontoolkit/models/tmp_backbone.py	1970-01-01 01:00:00.000000000 +0100
+++ revised/models/tmp_backbone.py	2021-06-24 23:02:39.836058296 +0100
@@ -0,0 +1,218 @@
+#!/usr/bin/env python
+# Copyright 2018 Division of Medical Image Computing, German Cancer Research Center (DKFZ).
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import torch.nn as nn
+import torch.nn.functional as F
+import torch
+
+
+class FPN(nn.Module):
+    """
+    Feature Pyramid Network from https://arxiv.org/pdf/1612.03144.pdf with options for modifications.
+    by default is constructed with Pyramid levels P2, P3, P4, P5.
+    """
+    def __init__(self, cf, conv, operate_stride1=False):
+        """
+        from configs:
+        :param input_channels: number of channel dimensions in input data.
+        :param start_filts:  number of feature_maps in first layer. rest is scaled accordingly.
+        :param out_channels: number of feature_maps for output_layers of all levels in decoder.
+        :param conv: instance of custom conv class containing the dimension info.
+        :param res_architecture: string deciding whether to use "resnet50" or "resnet101".
+        :param operate_stride1: boolean flag. enables adding of Pyramid levels P1 (output stride 2) and P0 (output stride 1).
+        :param norm: string specifying type of feature map normalization. If None, no normalization is applied.
+        :param relu: string specifying type of nonlinearity. If None, no nonlinearity is applied.
+        :param sixth_pooling: boolean flag. enables adding of Pyramid level P6.
+        """
+        super(FPN, self).__init__()
+
+        self.start_filts = cf.start_filts
+        start_filts = self.start_filts
+        self.n_blocks = [3, 4, {"resnet50": 6, "resnet101": 23}[cf.res_architecture], 3]
+        self.block = ResBlock
+        self.block_expansion = 4
+        self.operate_stride1 = operate_stride1
+        self.sixth_pooling = cf.sixth_pooling
+        self.dim = conv.dim
+
+        if operate_stride1:
+            self.C0 = nn.Sequential(conv(cf.n_channels, start_filts, ks=3, pad=1, norm=cf.norm, relu=cf.relu),
+                                    conv(start_filts, start_filts, ks=3, pad=1, norm=cf.norm, relu=cf.relu))
+
+            self.C1 = conv(start_filts, start_filts, ks=7, stride=(2, 2, 1) if conv.dim == 3 else 2, pad=3, norm=cf.norm, relu=cf.relu)
+
+        else:
+            self.C1 = conv(cf.n_channels, start_filts, ks=7, stride=(2, 2, 1) if conv.dim == 3 else 2, pad=3, norm=cf.norm, relu=cf.relu)
+
+        start_filts_exp = start_filts * self.block_expansion
+
+        C2_layers = []
+        C2_layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+                         if conv.dim == 2 else nn.MaxPool3d(kernel_size=3, stride=(2, 2, 1), padding=1))
+        C2_layers.append(self.block(start_filts, start_filts, conv=conv, stride=1, norm=cf.norm, relu=cf.relu,
+                                    downsample=(start_filts, self.block_expansion, 1)))
+        for i in range(1, self.n_blocks[0]):
+            C2_layers.append(self.block(start_filts_exp, start_filts, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C2 = nn.Sequential(*C2_layers)
+
+        C3_layers = []
+        C3_layers.append(self.block(start_filts_exp, start_filts * 2, conv=conv, stride=2, norm=cf.norm, relu=cf.relu,
+                                    downsample=(start_filts_exp, 2, 2)))
+        for i in range(1, self.n_blocks[1]):
+            C3_layers.append(self.block(start_filts_exp * 2, start_filts * 2, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C3 = nn.Sequential(*C3_layers)
+
+        C4_layers = []
+        C4_layers.append(self.block(
+            start_filts_exp * 2, start_filts * 4, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 2, 2, 2)))
+        for i in range(1, self.n_blocks[2]):
+            C4_layers.append(self.block(start_filts_exp * 4, start_filts * 4, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C4 = nn.Sequential(*C4_layers)
+
+        C5_layers = []
+        C5_layers.append(self.block(
+            start_filts_exp * 4, start_filts * 8, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 4, 2, 2)))
+        for i in range(1, self.n_blocks[3]):
+            C5_layers.append(self.block(start_filts_exp * 8, start_filts * 8, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C5 = nn.Sequential(*C5_layers)
+
+        if self.sixth_pooling:
+            C6_layers = []
+            C6_layers.append(self.block(
+                start_filts_exp * 8, start_filts * 16, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 8, 2, 2)))
+            for i in range(1, self.n_blocks[3]):
+                C6_layers.append(self.block(start_filts_exp * 16, start_filts * 16, conv=conv, norm=cf.norm, relu=cf.relu))
+            self.C6 = nn.Sequential(*C6_layers)
+
+        if conv.dim == 2:
+            self.P1_upsample = Interpolate(scale_factor=2, mode='bilinear')
+            self.P2_upsample = Interpolate(scale_factor=2, mode='bilinear')
+        else:
+            self.P1_upsample = Interpolate(scale_factor=(2, 2, 1), mode='trilinear')
+            self.P2_upsample = Interpolate(scale_factor=(2, 2, 1), mode='trilinear')
+
+        self.out_channels = cf.end_filts
+        self.P5_conv1 = conv(start_filts*32 + cf.n_latent_dims, self.out_channels, ks=1, stride=1, relu=None) #
+        self.P4_conv1 = conv(start_filts*16, self.out_channels, ks=1, stride=1, relu=None)
+        self.P3_conv1 = conv(start_filts*8, self.out_channels, ks=1, stride=1, relu=None)
+        self.P2_conv1 = conv(start_filts*4, self.out_channels, ks=1, stride=1, relu=None)
+        self.P1_conv1 = conv(start_filts, self.out_channels, ks=1, stride=1, relu=None)
+
+        if operate_stride1:
+            self.P0_conv1 = conv(start_filts, self.out_channels, ks=1, stride=1, relu=None)
+            self.P0_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+        self.P1_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P2_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P3_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P4_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P5_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+        if self.sixth_pooling:
+            self.P6_conv1 = conv(start_filts * 64, self.out_channels, ks=1, stride=1, relu=None)
+            self.P6_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+
+    def forward(self, x):
+        """
+        :param x: input image of shape (b, c, y, x, (z))
+        :return: list of output feature maps per pyramid level, each with shape (b, c, y, x, (z)).
+        """
+        if self.operate_stride1:
+            c0_out = self.C0(x)
+        else:
+            c0_out = x
+
+        c1_out = self.C1(c0_out)
+        c2_out = self.C2(c1_out)
+        c3_out = self.C3(c2_out)
+        c4_out = self.C4(c3_out)
+        c5_out = self.C5(c4_out)
+        if self.sixth_pooling:
+            c6_out = self.C6(c5_out)
+            p6_pre_out = self.P6_conv1(c6_out)
+            p5_pre_out = self.P5_conv1(c5_out) + F.interpolate(p6_pre_out, scale_factor=2)
+        else:
+            p5_pre_out = self.P5_conv1(c5_out)
+
+        p4_pre_out = self.P4_conv1(c4_out) + F.interpolate(p5_pre_out, scale_factor=2)
+        p3_pre_out = self.P3_conv1(c3_out) + F.interpolate(p4_pre_out, scale_factor=2)
+        p2_pre_out = self.P2_conv1(c2_out) + F.interpolate(p3_pre_out, scale_factor=2)
+
+        # plot feature map shapes for debugging.
+        # for ii in [c0_out, c1_out, c2_out, c3_out, c4_out, c5_out, c6_out]:
+        #     print ("encoder shapes:", ii.shape)
+        #
+        # for ii in [p6_out, p5_out, p4_out, p3_out, p2_out, p1_out]:
+        #     print("decoder shapes:", ii.shape)
+
+        p2_out = self.P2_conv2(p2_pre_out)
+        p3_out = self.P3_conv2(p3_pre_out)
+        p4_out = self.P4_conv2(p4_pre_out)
+        p5_out = self.P5_conv2(p5_pre_out)
+        out_list = [p2_out, p3_out, p4_out, p5_out]
+
+        if self.sixth_pooling:
+            p6_out = self.P6_conv2(p6_pre_out)
+            out_list.append(p6_out)
+
+        if self.operate_stride1:
+            p1_pre_out = self.P1_conv1(c1_out) + self.P2_upsample(p2_pre_out)
+            p0_pre_out = self.P0_conv1(c0_out) + self.P1_upsample(p1_pre_out)
+            # p1_out = self.P1_conv2(p1_pre_out) # usually not needed.
+            p0_out = self.P0_conv2(p0_pre_out)
+            out_list = [p0_out] + out_list
+
+        return out_list
+
+
+
+class ResBlock(nn.Module):
+
+    def __init__(self, start_filts, planes, conv, stride=1, downsample=None, norm=None, relu='relu'):
+        super(ResBlock, self).__init__()
+        self.conv1 = conv(start_filts, planes, ks=1, stride=stride, norm=norm, relu=relu)
+        self.conv2 = conv(planes, planes, ks=3, pad=1, norm=norm, relu=relu)
+        self.conv3 = conv(planes, planes * 4, ks=1, norm=norm, relu=None)
+        self.relu = nn.ReLU(inplace=True) if relu == 'relu' else nn.LeakyReLU(inplace=True)
+        if downsample is not None:
+            self.downsample = conv(downsample[0], downsample[0] * downsample[1], ks=1, stride=downsample[2], norm=norm, relu=None)
+        else:
+            self.downsample = None
+        self.stride = stride
+
+    def forward(self, x):
+        residual = x
+        out = self.conv1(x)
+        out = self.conv2(out)
+        out = self.conv3(out)
+        if self.downsample:
+            residual = self.downsample(x)
+        out += residual
+        out = self.relu(out)
+        return out
+
+
+class Interpolate(nn.Module):
+    def __init__(self, scale_factor, mode):
+        super(Interpolate, self).__init__()
+        self.interp = nn.functional.interpolate
+        self.scale_factor = scale_factor
+        self.mode = mode
+
+    def forward(self, x):
+        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)
+        return x
\ No newline at end of file
diff -ruN medicaldetectiontoolkit/models/tmp_model.py revised/models/tmp_model.py
--- medicaldetectiontoolkit/models/tmp_model.py	1970-01-01 01:00:00.000000000 +0100
+++ revised/models/tmp_model.py	2021-06-24 23:02:39.831058296 +0100
@@ -0,0 +1,214 @@
+#!/usr/bin/env python
+# Copyright 2018 Division of Medical Image Computing, German Cancer Research Center (DKFZ).
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""
+Unet-like Backbone architecture, with non-parametric heuristics for box detection on semantic segmentation outputs.
+"""
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from scipy.ndimage.measurements import label as lb
+import numpy as np
+import utils.exp_utils as utils
+import utils.model_utils as mutils
+
+
+class net(nn.Module):
+
+    def __init__(self, cf, logger):
+
+        super(net, self).__init__()
+        self.cf = cf
+        self.logger = logger
+        backbone = utils.import_module('bbone', cf.backbone_path)
+        conv = mutils.NDConvGenerator(cf.dim)
+
+        # set operate_stride1=True to generate a unet-like FPN.)
+        self.fpn = backbone.FPN(cf, conv, operate_stride1=True).cuda()
+        self.conv_final = conv(cf.end_filts, cf.num_seg_classes, ks=1, pad=0, norm=cf.norm, relu=None)
+
+        if self.cf.weight_init is not None:
+            logger.info("using pytorch weight init of type {}".format(self.cf.weight_init))
+            mutils.initialize_weights(self)
+        else:
+            logger.info("using default pytorch weight init")
+
+
+    def forward(self, x):
+        """
+        forward pass of network.
+        :param x: input image. shape (b, c, y, x, (z))
+        :return: seg_logits: shape (b, n_classes, y, x, (z))
+        :return: out_box_coords: list over n_classes. elements are arrays(b, n_rois, (y1, x1, y2, x2, (z1), (z2)))
+        :return: out_max_scores: list over n_classes. elements are arrays(b, n_rois)
+        """
+
+        out_features = self.fpn(x)[0]
+        seg_logits = self.conv_final(out_features)
+        out_box_coords, out_max_scores = [], []
+        smax = F.softmax(seg_logits, dim=1).detach().cpu().data.numpy()
+
+        for cl in range(1, len(self.cf.class_dict.keys()) + 1):
+            max_scores = [[] for _ in range(x.shape[0])]
+            hard_mask = np.copy(smax).argmax(1)
+            hard_mask[hard_mask != cl] = 0
+            hard_mask[hard_mask == cl] = 1
+            # perform connected component analysis on argmaxed predictions,
+            # draw boxes around components and return coordinates.
+            box_coords, rois = get_coords(hard_mask, self.cf.n_roi_candidates, self.cf.dim)
+
+            # for each object, choose the highest softmax score (in the respective class)
+            # of all pixels in the component as object score.
+            for bix, broi in enumerate(rois):
+                for nix, nroi in enumerate(broi):
+                    component_score = np.max(smax[bix, cl][nroi > 0]) if self.cf.aggregation_operation == 'max' \
+                        else np.median(smax[bix, cl][nroi > 0])
+                    max_scores[bix].append(component_score)
+            out_box_coords.append(box_coords)
+            out_max_scores.append(max_scores)
+        return seg_logits, out_box_coords, out_max_scores
+
+
+    def train_forward(self, batch, **kwargs):
+        """
+        train method (also used for validation monitoring). wrapper around forward pass of network. prepares input data
+        for processing, computes losses, and stores outputs in a dictionary.
+        :param batch: dictionary containing 'data', 'seg', etc.
+        :param kwargs:
+        :return: results_dict: dictionary with keys:
+                'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:
+                        [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]
+                'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes]
+                'monitor_values': dict of values to be monitored.
+        """
+        img = batch['data']
+        seg = batch['seg']
+        var_img = torch.FloatTensor(img).cuda()
+        var_seg = torch.FloatTensor(seg).cuda().long()
+        var_seg_ohe = torch.FloatTensor(mutils.get_one_hot_encoding(seg, self.cf.num_seg_classes)).cuda()
+        results_dict = {}
+        seg_logits, box_coords, max_scores = self.forward(var_img)
+
+        results_dict['boxes'] = [[] for _ in range(img.shape[0])]
+        for cix in range(len(self.cf.class_dict.keys())):
+            for bix in range(img.shape[0]):
+                for rix in range(len(max_scores[cix][bix])):
+                    if max_scores[cix][bix][rix] > self.cf.detection_min_confidence:
+                        results_dict['boxes'][bix].append({'box_coords': np.copy(box_coords[cix][bix][rix]),
+                                                           'box_score': max_scores[cix][bix][rix],
+                                                           'box_pred_class_id': cix + 1,  # add 0 for background.
+                                                           'box_type': 'det'})
+
+
+        for bix in range(img.shape[0]):
+            for tix in range(len(batch['bb_target'][bix])):
+                results_dict['boxes'][bix].append({'box_coords': batch['bb_target'][bix][tix],
+                                                   'box_label': batch['roi_labels'][bix][tix],
+                                                   'box_type': 'gt'})
+
+        # compute segmentation loss as either weighted cross entropy, dice loss, or the sum of both.
+        loss = torch.FloatTensor([0]).cuda()
+        if self.cf.seg_loss_mode == 'dice' or self.cf.seg_loss_mode == 'dice_wce':
+            loss += 1 - mutils.batch_dice(F.softmax(seg_logits, dim=1), var_seg_ohe,
+                                          false_positive_weight=float(self.cf.fp_dice_weight))
+
+        if self.cf.seg_loss_mode == 'wce' or self.cf.seg_loss_mode == 'dice_wce':
+            loss += F.cross_entropy(seg_logits, var_seg[:, 0], weight=torch.tensor(self.cf.wce_weights).float().cuda())
+
+        results_dict['seg_preds'] = np.argmax(F.softmax(seg_logits, 1).cpu().data.numpy(), 1)[:, np.newaxis]
+        results_dict['torch_loss'] = loss
+        results_dict['monitor_values'] = {'loss': loss.item()}
+        results_dict['logger_string'] = "loss: {0:.2f}".format(loss.item())
+
+
+        return results_dict
+
+
+    def test_forward(self, batch, **kwargs):
+        """
+        test method. wrapper around forward pass of network without usage of any ground truth information.
+        prepares input data for processing and stores outputs in a dictionary.
+        :param batch: dictionary containing 'data'
+        :param kwargs:
+        :return: results_dict: dictionary with keys:
+               'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:
+                       [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]
+               'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes]
+        """
+        img = batch['data']
+        var_img = torch.FloatTensor(img).cuda()
+        seg_logits, box_coords, max_scores = self.forward(var_img)
+
+        results_dict = {}
+        results_dict['boxes'] = [[] for _ in range(img.shape[0])]
+        for cix in range(len(self.cf.class_dict.keys())):
+            for bix in range(img.shape[0]):
+                for rix in range(len(max_scores[cix][bix])):
+                    if max_scores[cix][bix][rix] > self.cf.detection_min_confidence:
+                        results_dict['boxes'][bix].append({'box_coords': np.copy(box_coords[cix][bix][rix]),
+                                                           'box_score': max_scores[cix][bix][rix],
+                                                           'box_pred_class_id': cix + 1,  # add 0 for background.
+                                                           'box_type': 'det'})
+
+        results_dict['seg_preds'] = np.argmax(F.softmax(seg_logits, 1).cpu().data.numpy(), 1)[:, np.newaxis].astype('uint8')
+        return results_dict
+
+
+
+def get_coords(binary_mask, n_components, dim):
+    """
+    loops over batch to perform connected component analysis on binary input mask. computes box coordiantes around
+    n_components - biggest components (rois).
+    :param binary_mask: (b, y, x, (z)). binary mask for one specific foreground class.
+    :param n_components: int. number of components to extract per batch element and class.
+    :return: coords (b, n, (y1, x1, y2, x2, (z1), (z2))
+    :return: batch_components (b, n, (y1, x1, y2, x2, (z1), (z2))
+    """
+    binary_mask = binary_mask.astype('uint8')
+    batch_coords = []
+    batch_components = []
+    for ix, b in enumerate(binary_mask):
+        clusters, n_cands = lb(b)  # peforms connected component analysis.
+        uniques, counts = np.unique(clusters, return_counts=True)
+        # only keep n_components largest components.
+        keep_uniques = uniques[1:][np.argsort(counts[1:])[::-1]][:n_components]
+        # separate clusters and concat.
+        p_components = np.array([(clusters == ii) * 1 for ii in keep_uniques])
+        p_coords = []
+        if p_components.shape[0] > 0:
+            for roi in p_components:
+                mask_ixs = np.argwhere(roi != 0)
+
+                # get coordinates around component.
+                roi_coords = [np.min(mask_ixs[:, 0]) - 1, np.min(mask_ixs[:, 1]) - 1, np.max(mask_ixs[:, 0]) + 1,
+                              np.max(mask_ixs[:, 1]) + 1]
+                if dim == 3:
+                    roi_coords += [np.min(mask_ixs[:, 2]), np.max(mask_ixs[:, 2])+1]
+                p_coords.append(roi_coords)
+
+            p_coords = np.array(p_coords)
+
+            # clip coords.
+            p_coords[p_coords < 0] = 0
+            p_coords[:, :4][p_coords[:, :4] > binary_mask.shape[-2]] = binary_mask.shape[-2]
+            if dim == 3:
+                p_coords[:, 4:][p_coords[:, 4:] > binary_mask.shape[-1]] = binary_mask.shape[-1]
+
+        batch_coords.append(p_coords)
+        batch_components.append(p_components)
+    return batch_coords, batch_components
+
diff -ruN medicaldetectiontoolkit/predictor.py revised/predictor.py
--- medicaldetectiontoolkit/predictor.py	2021-06-27 22:37:22.711564669 +0100
+++ revised/predictor.py	2021-06-24 23:16:11.700027755 +0100
@@ -145,24 +145,33 @@
             self.rank_ix = str(rank_ix)  # get string of current rank for unique patch ids.
 
             with torch.no_grad():
-                for _ in range(batch_gen['n_test']):
+                print("batch_gen n_test %d" % batch_gen["n_test"])
+                for test_num in range(batch_gen['n_test']):
 
                     batch = next(batch_gen['test'])
 
                     # store batch info in patient entry of results dict.
                     if rank_ix == 0:
+                    # if batch["pid"] not in dict_of_patient_results:
                         dict_of_patient_results[batch['pid']] = {}
                         dict_of_patient_results[batch['pid']]['results_list'] = []
                         dict_of_patient_results[batch['pid']]['patient_bb_target'] = batch['patient_bb_target']
                         dict_of_patient_results[batch['pid']]['patient_roi_labels'] = batch['patient_roi_labels']
+                        dict_of_patient_results[batch['pid']]['seg_results_list'] = []
+                        dict_of_patient_results[batch['pid']]['inst_results_list'] = []
 
                     # call prediction pipeline and store results in dict.
                     results_dict = self.predict_patient(batch)
                     dict_of_patient_results[batch['pid']]['results_list'].append(results_dict['boxes'])
-
+                    dict_of_patient_results[batch['pid']]['seg_results_list'].append(results_dict['seg_preds'])
+                    dict_of_patient_results[batch['pid']]['inst_results_list'].append(results_dict['inst_preds'])
+                    print("recording pid %d seg shape %s" % (batch["pid"], results_dict['seg_preds'].shape))
+                    print("num boxes %d inst segs %d" % (len(results_dict["boxes"][0]), len(results_dict["inst_preds"])))
 
         self.logger.info('finished predicting test set. starting post-processing of predictions.')
         list_of_results_per_patient = []
+        results_per_patient_seg = {}
+        results_per_patient_inst = {}
 
         # loop over patients again to flatten results across epoch predictions.
         # if provided, add ground truth boxes for evaluation.
@@ -175,9 +184,21 @@
                                      for batch_instance in range(len(tmp_ens_list[0]))]
 
             # TODO return for instance segmentation:
+            tmp_ens_list_seg = p_dict['seg_results_list']
             # results_dict['seg_preds'] = np.mean(results_dict['seg_preds'], 1)[:, None]
-            # results_dict['seg_preds'] = np.array([[item for d in tmp_ens_list for item in d['seg_preds'][batch_instance]]
-            #                                       for batch_instance in range(len(tmp_ens_list[0]['boxes']))])
+
+            # tmp_ens_list = list len 1, tmp_ens_list[0] = list len 1 (?), tmp_ens_list[0][0] = list len 960 ?
+            results_dict['seg_preds'] = np.array([[item for d in tmp_ens_list_seg for item in d[batch_instance]]
+                                                 for batch_instance in range(len(tmp_ens_list_seg[0]))])
+
+            # assume first dimension are instances of segmentation (?)
+            # flatten second dimension assuming it is based on augmentations.
+            # results_dict['seg_preds'] = np.concatenate(tmp_ens_list_seg, axis=0).mean(axis=1, keepdims=True)
+
+            # results_dict["inst_preds"] = np.concatenate(p_dict['inst_results_list'], axis=0)
+            results_dict["inst_preds"] = p_dict['inst_results_list']
+
+            # print("pid %d seg_preds shape %s inst_preds shape %s" % (pid, results_dict["seg_preds"].shape, results_dict["inst_preds"].shape))
 
             # add 3D ground truth boxes for evaluation.
             for b in range(p_dict['patient_bb_target'].shape[0]):
@@ -187,34 +208,136 @@
                                                      'box_type': 'gt'})
 
             list_of_results_per_patient.append([results_dict['boxes'], pid])
+            if self.cf.segmentation_method == "global":
+                if pid not in results_per_patient_seg:
+                    results_per_patient_seg[pid] = []
+                results_per_patient_seg[pid].append(results_dict['seg_preds'])
+            elif self.cf.segmentation_method == "instance":
+                if pid not in results_per_patient_inst:
+                    results_per_patient_inst[pid] = []
+                results_per_patient_inst[pid].append(results_dict['inst_preds'])
 
         # save out raw predictions.
         out_string = 'raw_pred_boxes_hold_out_list' if self.cf.hold_out_test_set else 'raw_pred_boxes_list'
         with open(os.path.join(self.cf.fold_dir, '{}.pickle'.format(out_string)), 'wb') as handle:
             pickle.dump(list_of_results_per_patient, handle)
 
+        # save out raw segmentations.
+        # find mean of segmentation predictions (don't use WBC)
+        # for pid, segs in results_per_patient_seg.items():
+        #     # merged_segmentation = np.mean(segs, axis=0)
+        #     merged_segmentation = np.concatenate(segs, axis=0)
+        #
+        #     # print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #     print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #
+        #     out_file = os.path.join(self.cf.fold_dir, "seg_pid_%d.pickle" % pid)
+        #     with open(out_file, "wb") as fh:
+        #         pickle.dump(segs, fh)
+
+        # save out raw segmentations.
+        # find mean of segmentation predictions (don't use WBC)
+        # for pid, segs in results_per_patient_inst.items():
+        #     # merged_segmentation = np.mean(segs, axis=0)
+        #     merged_segmentation = np.concatenate(segs, axis=0)
+        #
+        #     # print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #     print("pid %d insts %d merged instseg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #
+        #     out_file = os.path.join(self.cf.fold_dir, "instseg_pid_%d.pickle" % pid)
+        #     with open(out_file, "wb") as fh:
+        #         pickle.dump(segs, fh)
+
         if return_results:
 
             # consolidate predictions.
             self.logger.info('applying wcs to test set predictions with iou = {} and n_ens = {}.'.format(
                 self.cf.wcs_iou, self.n_ens))
-            pool = Pool(processes=6)
-            mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.wcs_iou, self.n_ens] for ii in list_of_results_per_patient]
-            list_of_results_per_patient = pool.map(apply_wbc_to_patient, mp_inputs, chunksize=1)
-            pool.close()
-            pool.join()
+            run_parallel = False
+            mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.wcs_iou, self.n_ens] for ii in
+                         list_of_results_per_patient]
+            if run_parallel:
+                pool_processes = 6
+                pool = Pool(processes=pool_processes)
+                list_of_results_per_patient = pool.map(apply_wbc_to_patient, mp_inputs, chunksize=1)
+                pool.close()
+                pool.join()
+            else:
+                list_of_results_per_patient = [
+                    apply_wbc_to_patient(mp_input)
+                    for mp_input in mp_inputs
+                ]
 
             # merge 2D boxes to 3D cubes. (if model predicts 2D but evaluation is run in 3D)
             if self.cf.merge_2D_to_3D_preds:
                 self.logger.info('applying 2Dto3D merging to test set predictions with iou = {}.'.format(self.cf.merge_3D_iou))
-                pool = Pool(processes=6)
+                pool = Pool(processes=pool_processes)
                 mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.merge_3D_iou] for ii in list_of_results_per_patient]
                 list_of_results_per_patient = pool.map(merge_2D_to_3D_preds_per_patient, mp_inputs, chunksize=1)
                 pool.close()
                 pool.join()
 
+            # combine instance segmentations based on associations in reduced boxes
+            #instsegs_per_patient = {}
+            for p_list in list_of_results_per_patient:
+                pid = p_list[1]
+                patient_boxes = [b for x in p_list[0] for b in x]
+
+                if self.cf.segmentation_method == "global":
+                    # combine segmentations from various ensemble models and instances
+                    # augmentation into a single segmentation array
+                    seg_arrays = [y for x in results_per_patient_seg[pid] for y in x]
+                    merged_segs = np.mean([np.mean(x, axis=0) for x in seg_arrays], axis=0)
+                    # merged_segs just has the spatial dimensions (x,y,z), stored format is expected
+                    # to include additional dimensions for example for instances, so add dimensions
+                    merged_segs = merged_segs[None, None, :, :, :]
+                    print("pid %d mean %f max %f min %f dtype %s" % (pid, merged_segs.mean(), merged_segs.max(), merged_segs.min(), merged_segs.dtype))
+                elif self.cf.segmentation_method == "instance":
+                    # flatten to get instance segmentation arrays
+                    inst_seg_arrays = [z for x in results_per_patient_inst[pid] for y in x for z in y]
+                    seg_array_sizes = [x.shape[0] for x in inst_seg_arrays]
+                    indices = np.cumsum([0] + seg_array_sizes)
+
+                    merged_segs = np.array([
+                        self.merge_segmentation(inst_seg_arrays, indices, this_box["box_assocs"])
+                        for this_box in patient_boxes
+                    ])
+                    print("pid %d insts %d merged insts %s mean %f max %f min %f dtype %s" % (pid, indices[-1], merged_segs.shape, merged_segs.mean(), merged_segs.max(), merged_segs.min(), merged_segs.dtype))
+
+                #instsegs_per_patient[pid] = merged_segs
+
+                # write out merged instance segmentations to file
+                if self.cf.segmentation_method is not None:
+
+                    out_file = os.path.join(self.cf.fold_dir, "instseg_pid_%d.pickle" % pid)
+                    with open(out_file, "wb") as fh:
+                        pickle.dump(merged_segs, fh)
+
             return list_of_results_per_patient
 
+    def merge_segmentation(self, inst_seg_arrays, array_indices, box_assocs):
+        """
+        Merge multiple instance segmentations into combined segmentations, based on associations between
+        boxes after performing weighted box clustering.
+        :param inst_seg_arrays: List containing a number of instance segmentation arrays
+        :param array_indices: Array of cumulative size of each array
+        :param box_assocs: List of associated input boxes and weights corresponding with a given output box
+        """
+        def get_input_seg(idx):
+            array_idx = np.where((idx >= array_indices[:-1]) & (idx < array_indices[1:]))[0][0]
+            array_inst_idx = idx - array_indices[array_idx]
+            return inst_seg_arrays[array_idx][array_inst_idx]
+
+        # get segments from indexes
+        weighted_segs = []
+        weight_sum = 0.
+        for input_idx, weight in box_assocs:
+            input_seg = get_input_seg(input_idx) # shape (x,y,z)
+            weighted_segs.append(input_seg * weight)
+            weight_sum += weight
+
+        merged_seg = np.sum(weighted_segs, axis=0) / weight_sum
+        return merged_seg
 
     def load_saved_predictions(self, apply_wbc=False):
         """
@@ -361,6 +484,11 @@
                                  for batch_instance in range(org_img_shape[0])]
         results_dict['seg_preds'] = np.array([[item for d in results_list for item in d['seg_preds'][batch_instance]]
                                               for batch_instance in range(org_img_shape[0])])
+        # todo: remove check and allow varying batch_instance size (?)
+        assert org_img_shape[0] == 1
+        # results_dict['inst_preds'] = np.concatenate([d["inst_preds"] for d in results_list], axis=0)
+        results_dict['inst_preds'] = [d["inst_preds"] for d in results_list]
+
         if self.mode == 'val':
             results_dict['monitor_values'] = results_list[0]['monitor_values']
 
@@ -394,11 +522,22 @@
             # counts patch instances per pixel-position.
             patch_overlap_map = np.zeros_like(out_seg_preds, dtype='uint8')
 
+            # produce map of instance segmentations, in space of original image
+            # find total number of instances (corresponding with boxes)
+            num_insts = np.sum([x.shape[0] for x in patches_dict["seg_preds"]])
+            instance_segs = np.zeros((num_insts,) + batch["original_img_shape"][2:], dtype=np.float16)
+
             #unmold segmentation outputs. loop over patches.
+            inst_count = 0
             for pix, pc in enumerate(patch_crops):
+                this_instance_segs = patches_dict['seg_preds'][pix]
+                merged_segment = np.mean(this_instance_segs, axis=0, keepdims=True) # (1,x,y,z)
                 if self.cf.dim == 3:
-                    out_seg_preds[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += patches_dict['seg_preds'][pix][None]
+                    out_seg_preds[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += merged_segment #patches_dict['seg_preds'][pix][None]
                     patch_overlap_map[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += 1
+
+                    instance_segs[inst_count:inst_count+this_instance_segs.shape[0], pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += this_instance_segs
+                    inst_count += this_instance_segs.shape[0]
                 else:
                     out_seg_preds[pc[4]:pc[5], :, pc[0]:pc[1], pc[2]:pc[3], ] += patches_dict['seg_preds'][pix]
                     patch_overlap_map[pc[4]:pc[5], :, pc[0]:pc[1], pc[2]:pc[3], ] += 1
@@ -406,6 +545,7 @@
             # take mean in overlapping areas.
             out_seg_preds[patch_overlap_map > 0] /= patch_overlap_map[patch_overlap_map > 0]
             results_dict['seg_preds'] = out_seg_preds
+            results_dict["inst_preds"] = instance_segs
 
             # unmold box outputs. loop over patches.
             for pix, pc in enumerate(patch_crops):
@@ -497,7 +637,7 @@
             results_dict = {}
             # flatten out batch elements from chunks ([chunk, chunk] -> [b, b, b, b, ...])
             results_dict['boxes'] = [item for d in chunk_dicts for item in d['boxes']]
-            results_dict['seg_preds'] = np.array([item for d in chunk_dicts for item in d['seg_preds']])
+            results_dict['seg_preds'] = [item for d in chunk_dicts for item in d['seg_preds']]
 
             if self.mode == 'val':
                 # estimate metrics by mean over batch_chunks. Most similar to training metrics.
@@ -536,13 +676,14 @@
             box_patch_id = np.array([b[1]['patch_id'] for b in boxes])
 
             if 0 not in box_scores.shape:
-                keep_scores, keep_coords = weighted_box_clustering(
+                keep_scores, keep_coords, keep_assocs = weighted_box_clustering(
                     np.concatenate((box_coords, box_scores[:, None], box_center_factor[:, None],
                                     box_n_overlaps[:, None]), axis=1), box_patch_id, wcs_iou, n_ens)
 
                 for boxix in range(len(keep_scores)):
                     out_patient_results_list[bix].append({'box_type': 'det', 'box_coords': keep_coords[boxix],
-                                             'box_score': keep_scores[boxix], 'box_pred_class_id': cl})
+                                             'box_score': keep_scores[boxix], 'box_pred_class_id': cl,
+                                                          'box_assocs': keep_assocs[boxix]})
 
         # add gt boxes back to new output list.
         out_patient_results_list[bix].extend([box for box in b if box['box_type'] == 'gt'])
@@ -608,6 +749,7 @@
     :param n_ens: number of models, that are ensembled. (-> number of expected predicitions per position)
     :return: keep_scores: (n_keep)  new scores of boxes to be kept.
     :return: keep_coords: (n_keep, (y1, x1, y2, x2, (z1), (z2)) new coordinates of boxes to be kept.
+    :return: keep_assocs: (n_keep, [input_idx, score]) list of associated input boxes and scores for each returned box
     """
     dim = 2 if dets.shape[1] == 7 else 3
     y1 = dets[:, 0]
@@ -631,6 +773,7 @@
     keep = []
     keep_scores = []
     keep_coords = []
+    keep_assocs = []
 
     while order.size > 0:
         i = order[0]  # higehst scoring element
@@ -698,12 +841,13 @@
         if avg_score > 0.01:
             keep_scores.append(avg_score)
             keep_coords.append(avg_coords)
+            keep_assocs.append(list(zip(matches, match_scores)))
 
         # get index of all elements that were not matched and discard all others.
         inds = np.where(ovr <= thresh)[0]
         order = order[inds]
 
-    return keep_scores, keep_coords
+    return keep_scores, keep_coords, keep_assocs
 
 
 
diff -ruN medicaldetectiontoolkit/README.md revised/README.md
--- medicaldetectiontoolkit/README.md	2021-06-27 22:37:22.686564670 +0100
+++ revised/README.md	2020-10-18 20:11:10.636575637 +0100
@@ -21,7 +21,7 @@
 <br/>
 [1] He, Kaiming, et al.  <a href="https://arxiv.org/abs/1703.06870">"Mask R-CNN"</a> ICCV, 2017<br>
 [2] Lin, Tsung-Yi, et al.  <a href="https://arxiv.org/abs/1708.02002">"Focal Loss for Dense Object Detection"</a> TPAMI, 2018.<br>
-[3] Jaeger, Paul et al. <a href="https://ml4health.github.io/2019/pdf/232_ml4h_preprint.pdf"> "Retina U-Net: Embarrassingly Simple Exploitation
+[3] Jaeger, Paul et al. <a href="http://arxiv.org/abs/1811.08661"> "Retina U-Net: Embarrassingly Simple Exploitation
 of Segmentation Supervision for Medical Object Detection" </a>, 2018
 
 [5] https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py<br/>
diff -ruN medicaldetectiontoolkit/requirements.txt revised/requirements.txt
--- medicaldetectiontoolkit/requirements.txt	2021-06-27 22:37:22.711564669 +0100
+++ revised/requirements.txt	2020-10-18 20:25:32.259622834 +0100
@@ -10,7 +10,7 @@
 networkx==2.4
 numpy==1.15.3
 pandas==0.23.4
-Pillow==8.1.1
+Pillow==7.1.0
 pycparser==2.19
 pyparsing==2.4.5
 python-dateutil==2.8.1
@@ -22,6 +22,6 @@
 six==1.13.0
 sklearn==0.0
 threadpoolctl==1.1.0
-torch==0.4.1
+#torch==0.4.1
 traceback2==1.4.0
 unittest2==1.1.0
