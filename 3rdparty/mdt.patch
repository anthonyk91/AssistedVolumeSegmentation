diff -ruN medicaldetectiontoolkit/default_configs.py revised/default_configs.py
--- medicaldetectiontoolkit/default_configs.py	2021-06-28 22:59:37.733392830 +0100
+++ revised/default_configs.py	2021-06-24 21:53:56.158213420 +0100
@@ -137,4 +137,6 @@
         # for probabilistic detection
         self.n_latent_dims = 0
 
-
+        # choose between segmentation methods for returning segmentation output.  None for no output, "global" for a
+        # single global segmentation and "instance" for instance segmentation
+        self.segmentation_method = None
diff -ruN medicaldetectiontoolkit/exec.py revised/exec.py
--- medicaldetectiontoolkit/exec.py	2021-06-28 22:59:37.733392830 +0100
+++ revised/exec.py	2021-06-20 23:35:17.403058480 +0100
@@ -86,13 +86,19 @@
             net.eval()
             if cf.do_validation:
                 val_results_list = []
-                val_predictor = Predictor(cf, net, logger, mode='val')
-                for _ in range(batch_gen['n_val']):
+                if cf.val_mode == 'val_patient':
+                    val_predictor = Predictor(cf, net, logger, mode='val')
+                    num_val = batch_gen['n_val']
+                elif cf.val_mode == 'val_sampling':
+                    num_val = cf.num_val_batches
+                for vix in range(num_val):
                     batch = next(batch_gen[cf.val_mode])
                     if cf.val_mode == 'val_patient':
                         results_dict = val_predictor.predict_patient(batch)
                     elif cf.val_mode == 'val_sampling':
                         results_dict = net.train_forward(batch, is_validation=True)
+                    logger.info('val. batch {0}/{1} (ep. {2}) ||'
+                                .format(vix + 1, num_val, epoch) + results_dict['logger_string'])
                     val_results_list.append([results_dict['boxes'], batch['pid']])
                     monitor_metrics['val']['monitor_values'][epoch].append(results_dict['monitor_values'])
 
diff -ruN medicaldetectiontoolkit/experiments/lidc_exp/configs.py revised/experiments/lidc_exp/configs.py
--- medicaldetectiontoolkit/experiments/lidc_exp/configs.py	2021-06-28 22:59:37.733392830 +0100
+++ revised/experiments/lidc_exp/configs.py	2020-10-18 22:08:02.220959710 +0100
@@ -52,7 +52,8 @@
         # path to preprocessed data.
         self.pp_name = 'lidc_mdt'
         self.input_df_name = 'info_df.pickle'
-        self.pp_data_path = '/media/gregor/HDD2TB/data/lidc/{}'.format(self.pp_name)
+        self.pp_data_path = '/tmp/{}'.format(self.pp_name)
+        #self.pp_data_path = '/media/gregor/HDD2TB/data/lidc/{}'.format(self.pp_name)
         self.pp_test_data_path = self.pp_data_path #change if test_data in separate folder.
 
         # settings for deployment in cloud.
@@ -331,4 +332,4 @@
             self.num_seg_classes = 3 if self.class_specific_seg_flag else 2
 
             if self.model == 'retina_unet':
-                self.operate_stride1 = True
\ No newline at end of file
+                self.operate_stride1 = True
diff -ruN medicaldetectiontoolkit/experiments/toy_exp/configs.py revised/experiments/toy_exp/configs.py
--- medicaldetectiontoolkit/experiments/toy_exp/configs.py	2021-06-28 22:59:37.733392830 +0100
+++ revised/experiments/toy_exp/configs.py	2020-10-18 22:19:39.761997919 +0100
@@ -28,7 +28,8 @@
         #    Preprocessing      #
         #########################
 
-        self.root_dir = '/home/gregor/datasets/toy_mdt'
+        #self.root_dir = '/home/gregor/datasets/toy_mdt'
+        self.root_dir = '/tmp'
 
         #########################
         #         I/O           #
@@ -51,7 +52,8 @@
 
         # choose one of the 3 toy experiments described in https://arxiv.org/pdf/1811.08661.pdf
         # one of ['donuts_shape', 'donuts_pattern', 'circles_scale'].
-        toy_mode = 'donuts_shape_noise'
+        #toy_mode = 'donuts_shape_noise'
+        toy_mode = 'donuts_shape'
 
 
         # path to preprocessed data.
diff -ruN medicaldetectiontoolkit/models/mrcnn.py revised/models/mrcnn.py
--- medicaldetectiontoolkit/models/mrcnn.py	2021-06-28 22:59:37.734392830 +0100
+++ revised/models/mrcnn.py	2021-06-20 16:08:20.361589525 +0100
@@ -257,6 +257,7 @@
     :param target_class_ids: (n_sampled_rois)
     :return: loss: torch 1D tensor.
     """
+    #print("target class ids", target_class_ids)
     if 0 not in torch.nonzero(target_class_ids > 0).size():
         positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]
         positive_roi_class_ids = target_class_ids[positive_roi_ix].long()
@@ -778,8 +779,12 @@
                     if cf.dim == 2 else mutils.unmold_mask_3D(masks[i], boxes[i], permuted_image_shape))
             # if masks are returned, take max over binary full masks of all predictions in this image.
             # right now only binary masks for plotting/monitoring. for instance segmentation return all proposal maks.
-            final_masks = np.max(np.array(full_masks), 0) if len(full_masks) > 0 else np.zeros(
-                (*permuted_image_shape[:-1],))
+            # final_masks = np.max(np.array(full_masks), 0) if len(full_masks) > 0 else np.zeros(
+            #     (*permuted_image_shape[:-1],))
+            if len(full_masks) > 0:
+                final_masks = np.array(full_masks)
+            else:
+                final_masks = np.zeros((*permuted_image_shape[:-1],))
 
             # add final perdictions to results.
             if 0 not in boxes.shape:
@@ -794,7 +799,8 @@
 
     # create and fill results dictionary.
     results_dict = {'boxes': box_results_list,
-                    'seg_preds': np.round(np.array(seg_preds))[:, np.newaxis].astype('uint8')}
+                    # 'seg_preds': np.round(np.array(seg_preds))[:, np.newaxis].astype('uint8')}
+                    'seg_preds': seg_preds} #np.round(np.array(seg_preds)).astype('uint8')}
 
     return results_dict
 
diff -ruN medicaldetectiontoolkit/models/tmp_backbone.py revised/models/tmp_backbone.py
--- medicaldetectiontoolkit/models/tmp_backbone.py	1970-01-01 01:00:00.000000000 +0100
+++ revised/models/tmp_backbone.py	2021-06-24 23:02:39.836058296 +0100
@@ -0,0 +1,218 @@
+#!/usr/bin/env python
+# Copyright 2018 Division of Medical Image Computing, German Cancer Research Center (DKFZ).
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import torch.nn as nn
+import torch.nn.functional as F
+import torch
+
+
+class FPN(nn.Module):
+    """
+    Feature Pyramid Network from https://arxiv.org/pdf/1612.03144.pdf with options for modifications.
+    by default is constructed with Pyramid levels P2, P3, P4, P5.
+    """
+    def __init__(self, cf, conv, operate_stride1=False):
+        """
+        from configs:
+        :param input_channels: number of channel dimensions in input data.
+        :param start_filts:  number of feature_maps in first layer. rest is scaled accordingly.
+        :param out_channels: number of feature_maps for output_layers of all levels in decoder.
+        :param conv: instance of custom conv class containing the dimension info.
+        :param res_architecture: string deciding whether to use "resnet50" or "resnet101".
+        :param operate_stride1: boolean flag. enables adding of Pyramid levels P1 (output stride 2) and P0 (output stride 1).
+        :param norm: string specifying type of feature map normalization. If None, no normalization is applied.
+        :param relu: string specifying type of nonlinearity. If None, no nonlinearity is applied.
+        :param sixth_pooling: boolean flag. enables adding of Pyramid level P6.
+        """
+        super(FPN, self).__init__()
+
+        self.start_filts = cf.start_filts
+        start_filts = self.start_filts
+        self.n_blocks = [3, 4, {"resnet50": 6, "resnet101": 23}[cf.res_architecture], 3]
+        self.block = ResBlock
+        self.block_expansion = 4
+        self.operate_stride1 = operate_stride1
+        self.sixth_pooling = cf.sixth_pooling
+        self.dim = conv.dim
+
+        if operate_stride1:
+            self.C0 = nn.Sequential(conv(cf.n_channels, start_filts, ks=3, pad=1, norm=cf.norm, relu=cf.relu),
+                                    conv(start_filts, start_filts, ks=3, pad=1, norm=cf.norm, relu=cf.relu))
+
+            self.C1 = conv(start_filts, start_filts, ks=7, stride=(2, 2, 1) if conv.dim == 3 else 2, pad=3, norm=cf.norm, relu=cf.relu)
+
+        else:
+            self.C1 = conv(cf.n_channels, start_filts, ks=7, stride=(2, 2, 1) if conv.dim == 3 else 2, pad=3, norm=cf.norm, relu=cf.relu)
+
+        start_filts_exp = start_filts * self.block_expansion
+
+        C2_layers = []
+        C2_layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+                         if conv.dim == 2 else nn.MaxPool3d(kernel_size=3, stride=(2, 2, 1), padding=1))
+        C2_layers.append(self.block(start_filts, start_filts, conv=conv, stride=1, norm=cf.norm, relu=cf.relu,
+                                    downsample=(start_filts, self.block_expansion, 1)))
+        for i in range(1, self.n_blocks[0]):
+            C2_layers.append(self.block(start_filts_exp, start_filts, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C2 = nn.Sequential(*C2_layers)
+
+        C3_layers = []
+        C3_layers.append(self.block(start_filts_exp, start_filts * 2, conv=conv, stride=2, norm=cf.norm, relu=cf.relu,
+                                    downsample=(start_filts_exp, 2, 2)))
+        for i in range(1, self.n_blocks[1]):
+            C3_layers.append(self.block(start_filts_exp * 2, start_filts * 2, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C3 = nn.Sequential(*C3_layers)
+
+        C4_layers = []
+        C4_layers.append(self.block(
+            start_filts_exp * 2, start_filts * 4, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 2, 2, 2)))
+        for i in range(1, self.n_blocks[2]):
+            C4_layers.append(self.block(start_filts_exp * 4, start_filts * 4, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C4 = nn.Sequential(*C4_layers)
+
+        C5_layers = []
+        C5_layers.append(self.block(
+            start_filts_exp * 4, start_filts * 8, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 4, 2, 2)))
+        for i in range(1, self.n_blocks[3]):
+            C5_layers.append(self.block(start_filts_exp * 8, start_filts * 8, conv=conv, norm=cf.norm, relu=cf.relu))
+        self.C5 = nn.Sequential(*C5_layers)
+
+        if self.sixth_pooling:
+            C6_layers = []
+            C6_layers.append(self.block(
+                start_filts_exp * 8, start_filts * 16, conv=conv, stride=2, norm=cf.norm, relu=cf.relu, downsample=(start_filts_exp * 8, 2, 2)))
+            for i in range(1, self.n_blocks[3]):
+                C6_layers.append(self.block(start_filts_exp * 16, start_filts * 16, conv=conv, norm=cf.norm, relu=cf.relu))
+            self.C6 = nn.Sequential(*C6_layers)
+
+        if conv.dim == 2:
+            self.P1_upsample = Interpolate(scale_factor=2, mode='bilinear')
+            self.P2_upsample = Interpolate(scale_factor=2, mode='bilinear')
+        else:
+            self.P1_upsample = Interpolate(scale_factor=(2, 2, 1), mode='trilinear')
+            self.P2_upsample = Interpolate(scale_factor=(2, 2, 1), mode='trilinear')
+
+        self.out_channels = cf.end_filts
+        self.P5_conv1 = conv(start_filts*32 + cf.n_latent_dims, self.out_channels, ks=1, stride=1, relu=None) #
+        self.P4_conv1 = conv(start_filts*16, self.out_channels, ks=1, stride=1, relu=None)
+        self.P3_conv1 = conv(start_filts*8, self.out_channels, ks=1, stride=1, relu=None)
+        self.P2_conv1 = conv(start_filts*4, self.out_channels, ks=1, stride=1, relu=None)
+        self.P1_conv1 = conv(start_filts, self.out_channels, ks=1, stride=1, relu=None)
+
+        if operate_stride1:
+            self.P0_conv1 = conv(start_filts, self.out_channels, ks=1, stride=1, relu=None)
+            self.P0_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+        self.P1_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P2_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P3_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P4_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+        self.P5_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+        if self.sixth_pooling:
+            self.P6_conv1 = conv(start_filts * 64, self.out_channels, ks=1, stride=1, relu=None)
+            self.P6_conv2 = conv(self.out_channels, self.out_channels, ks=3, stride=1, pad=1, relu=None)
+
+
+    def forward(self, x):
+        """
+        :param x: input image of shape (b, c, y, x, (z))
+        :return: list of output feature maps per pyramid level, each with shape (b, c, y, x, (z)).
+        """
+        if self.operate_stride1:
+            c0_out = self.C0(x)
+        else:
+            c0_out = x
+
+        c1_out = self.C1(c0_out)
+        c2_out = self.C2(c1_out)
+        c3_out = self.C3(c2_out)
+        c4_out = self.C4(c3_out)
+        c5_out = self.C5(c4_out)
+        if self.sixth_pooling:
+            c6_out = self.C6(c5_out)
+            p6_pre_out = self.P6_conv1(c6_out)
+            p5_pre_out = self.P5_conv1(c5_out) + F.interpolate(p6_pre_out, scale_factor=2)
+        else:
+            p5_pre_out = self.P5_conv1(c5_out)
+
+        p4_pre_out = self.P4_conv1(c4_out) + F.interpolate(p5_pre_out, scale_factor=2)
+        p3_pre_out = self.P3_conv1(c3_out) + F.interpolate(p4_pre_out, scale_factor=2)
+        p2_pre_out = self.P2_conv1(c2_out) + F.interpolate(p3_pre_out, scale_factor=2)
+
+        # plot feature map shapes for debugging.
+        # for ii in [c0_out, c1_out, c2_out, c3_out, c4_out, c5_out, c6_out]:
+        #     print ("encoder shapes:", ii.shape)
+        #
+        # for ii in [p6_out, p5_out, p4_out, p3_out, p2_out, p1_out]:
+        #     print("decoder shapes:", ii.shape)
+
+        p2_out = self.P2_conv2(p2_pre_out)
+        p3_out = self.P3_conv2(p3_pre_out)
+        p4_out = self.P4_conv2(p4_pre_out)
+        p5_out = self.P5_conv2(p5_pre_out)
+        out_list = [p2_out, p3_out, p4_out, p5_out]
+
+        if self.sixth_pooling:
+            p6_out = self.P6_conv2(p6_pre_out)
+            out_list.append(p6_out)
+
+        if self.operate_stride1:
+            p1_pre_out = self.P1_conv1(c1_out) + self.P2_upsample(p2_pre_out)
+            p0_pre_out = self.P0_conv1(c0_out) + self.P1_upsample(p1_pre_out)
+            # p1_out = self.P1_conv2(p1_pre_out) # usually not needed.
+            p0_out = self.P0_conv2(p0_pre_out)
+            out_list = [p0_out] + out_list
+
+        return out_list
+
+
+
+class ResBlock(nn.Module):
+
+    def __init__(self, start_filts, planes, conv, stride=1, downsample=None, norm=None, relu='relu'):
+        super(ResBlock, self).__init__()
+        self.conv1 = conv(start_filts, planes, ks=1, stride=stride, norm=norm, relu=relu)
+        self.conv2 = conv(planes, planes, ks=3, pad=1, norm=norm, relu=relu)
+        self.conv3 = conv(planes, planes * 4, ks=1, norm=norm, relu=None)
+        self.relu = nn.ReLU(inplace=True) if relu == 'relu' else nn.LeakyReLU(inplace=True)
+        if downsample is not None:
+            self.downsample = conv(downsample[0], downsample[0] * downsample[1], ks=1, stride=downsample[2], norm=norm, relu=None)
+        else:
+            self.downsample = None
+        self.stride = stride
+
+    def forward(self, x):
+        residual = x
+        out = self.conv1(x)
+        out = self.conv2(out)
+        out = self.conv3(out)
+        if self.downsample:
+            residual = self.downsample(x)
+        out += residual
+        out = self.relu(out)
+        return out
+
+
+class Interpolate(nn.Module):
+    def __init__(self, scale_factor, mode):
+        super(Interpolate, self).__init__()
+        self.interp = nn.functional.interpolate
+        self.scale_factor = scale_factor
+        self.mode = mode
+
+    def forward(self, x):
+        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)
+        return x
\ No newline at end of file
diff -ruN medicaldetectiontoolkit/models/tmp_model.py revised/models/tmp_model.py
--- medicaldetectiontoolkit/models/tmp_model.py	1970-01-01 01:00:00.000000000 +0100
+++ revised/models/tmp_model.py	2021-06-24 23:02:39.831058296 +0100
@@ -0,0 +1,214 @@
+#!/usr/bin/env python
+# Copyright 2018 Division of Medical Image Computing, German Cancer Research Center (DKFZ).
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""
+Unet-like Backbone architecture, with non-parametric heuristics for box detection on semantic segmentation outputs.
+"""
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from scipy.ndimage.measurements import label as lb
+import numpy as np
+import utils.exp_utils as utils
+import utils.model_utils as mutils
+
+
+class net(nn.Module):
+
+    def __init__(self, cf, logger):
+
+        super(net, self).__init__()
+        self.cf = cf
+        self.logger = logger
+        backbone = utils.import_module('bbone', cf.backbone_path)
+        conv = mutils.NDConvGenerator(cf.dim)
+
+        # set operate_stride1=True to generate a unet-like FPN.)
+        self.fpn = backbone.FPN(cf, conv, operate_stride1=True).cuda()
+        self.conv_final = conv(cf.end_filts, cf.num_seg_classes, ks=1, pad=0, norm=cf.norm, relu=None)
+
+        if self.cf.weight_init is not None:
+            logger.info("using pytorch weight init of type {}".format(self.cf.weight_init))
+            mutils.initialize_weights(self)
+        else:
+            logger.info("using default pytorch weight init")
+
+
+    def forward(self, x):
+        """
+        forward pass of network.
+        :param x: input image. shape (b, c, y, x, (z))
+        :return: seg_logits: shape (b, n_classes, y, x, (z))
+        :return: out_box_coords: list over n_classes. elements are arrays(b, n_rois, (y1, x1, y2, x2, (z1), (z2)))
+        :return: out_max_scores: list over n_classes. elements are arrays(b, n_rois)
+        """
+
+        out_features = self.fpn(x)[0]
+        seg_logits = self.conv_final(out_features)
+        out_box_coords, out_max_scores = [], []
+        smax = F.softmax(seg_logits, dim=1).detach().cpu().data.numpy()
+
+        for cl in range(1, len(self.cf.class_dict.keys()) + 1):
+            max_scores = [[] for _ in range(x.shape[0])]
+            hard_mask = np.copy(smax).argmax(1)
+            hard_mask[hard_mask != cl] = 0
+            hard_mask[hard_mask == cl] = 1
+            # perform connected component analysis on argmaxed predictions,
+            # draw boxes around components and return coordinates.
+            box_coords, rois = get_coords(hard_mask, self.cf.n_roi_candidates, self.cf.dim)
+
+            # for each object, choose the highest softmax score (in the respective class)
+            # of all pixels in the component as object score.
+            for bix, broi in enumerate(rois):
+                for nix, nroi in enumerate(broi):
+                    component_score = np.max(smax[bix, cl][nroi > 0]) if self.cf.aggregation_operation == 'max' \
+                        else np.median(smax[bix, cl][nroi > 0])
+                    max_scores[bix].append(component_score)
+            out_box_coords.append(box_coords)
+            out_max_scores.append(max_scores)
+        return seg_logits, out_box_coords, out_max_scores
+
+
+    def train_forward(self, batch, **kwargs):
+        """
+        train method (also used for validation monitoring). wrapper around forward pass of network. prepares input data
+        for processing, computes losses, and stores outputs in a dictionary.
+        :param batch: dictionary containing 'data', 'seg', etc.
+        :param kwargs:
+        :return: results_dict: dictionary with keys:
+                'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:
+                        [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]
+                'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes]
+                'monitor_values': dict of values to be monitored.
+        """
+        img = batch['data']
+        seg = batch['seg']
+        var_img = torch.FloatTensor(img).cuda()
+        var_seg = torch.FloatTensor(seg).cuda().long()
+        var_seg_ohe = torch.FloatTensor(mutils.get_one_hot_encoding(seg, self.cf.num_seg_classes)).cuda()
+        results_dict = {}
+        seg_logits, box_coords, max_scores = self.forward(var_img)
+
+        results_dict['boxes'] = [[] for _ in range(img.shape[0])]
+        for cix in range(len(self.cf.class_dict.keys())):
+            for bix in range(img.shape[0]):
+                for rix in range(len(max_scores[cix][bix])):
+                    if max_scores[cix][bix][rix] > self.cf.detection_min_confidence:
+                        results_dict['boxes'][bix].append({'box_coords': np.copy(box_coords[cix][bix][rix]),
+                                                           'box_score': max_scores[cix][bix][rix],
+                                                           'box_pred_class_id': cix + 1,  # add 0 for background.
+                                                           'box_type': 'det'})
+
+
+        for bix in range(img.shape[0]):
+            for tix in range(len(batch['bb_target'][bix])):
+                results_dict['boxes'][bix].append({'box_coords': batch['bb_target'][bix][tix],
+                                                   'box_label': batch['roi_labels'][bix][tix],
+                                                   'box_type': 'gt'})
+
+        # compute segmentation loss as either weighted cross entropy, dice loss, or the sum of both.
+        loss = torch.FloatTensor([0]).cuda()
+        if self.cf.seg_loss_mode == 'dice' or self.cf.seg_loss_mode == 'dice_wce':
+            loss += 1 - mutils.batch_dice(F.softmax(seg_logits, dim=1), var_seg_ohe,
+                                          false_positive_weight=float(self.cf.fp_dice_weight))
+
+        if self.cf.seg_loss_mode == 'wce' or self.cf.seg_loss_mode == 'dice_wce':
+            loss += F.cross_entropy(seg_logits, var_seg[:, 0], weight=torch.tensor(self.cf.wce_weights).float().cuda())
+
+        results_dict['seg_preds'] = np.argmax(F.softmax(seg_logits, 1).cpu().data.numpy(), 1)[:, np.newaxis]
+        results_dict['torch_loss'] = loss
+        results_dict['monitor_values'] = {'loss': loss.item()}
+        results_dict['logger_string'] = "loss: {0:.2f}".format(loss.item())
+
+
+        return results_dict
+
+
+    def test_forward(self, batch, **kwargs):
+        """
+        test method. wrapper around forward pass of network without usage of any ground truth information.
+        prepares input data for processing and stores outputs in a dictionary.
+        :param batch: dictionary containing 'data'
+        :param kwargs:
+        :return: results_dict: dictionary with keys:
+               'boxes': list over batch elements. each batch element is a list of boxes. each box is a dictionary:
+                       [[{box_0}, ... {box_n}], [{box_0}, ... {box_n}], ...]
+               'seg_preds': pixel-wise class predictions (b, 1, y, x, (z)) with values [0, n_classes]
+        """
+        img = batch['data']
+        var_img = torch.FloatTensor(img).cuda()
+        seg_logits, box_coords, max_scores = self.forward(var_img)
+
+        results_dict = {}
+        results_dict['boxes'] = [[] for _ in range(img.shape[0])]
+        for cix in range(len(self.cf.class_dict.keys())):
+            for bix in range(img.shape[0]):
+                for rix in range(len(max_scores[cix][bix])):
+                    if max_scores[cix][bix][rix] > self.cf.detection_min_confidence:
+                        results_dict['boxes'][bix].append({'box_coords': np.copy(box_coords[cix][bix][rix]),
+                                                           'box_score': max_scores[cix][bix][rix],
+                                                           'box_pred_class_id': cix + 1,  # add 0 for background.
+                                                           'box_type': 'det'})
+
+        results_dict['seg_preds'] = np.argmax(F.softmax(seg_logits, 1).cpu().data.numpy(), 1)[:, np.newaxis].astype('uint8')
+        return results_dict
+
+
+
+def get_coords(binary_mask, n_components, dim):
+    """
+    loops over batch to perform connected component analysis on binary input mask. computes box coordiantes around
+    n_components - biggest components (rois).
+    :param binary_mask: (b, y, x, (z)). binary mask for one specific foreground class.
+    :param n_components: int. number of components to extract per batch element and class.
+    :return: coords (b, n, (y1, x1, y2, x2, (z1), (z2))
+    :return: batch_components (b, n, (y1, x1, y2, x2, (z1), (z2))
+    """
+    binary_mask = binary_mask.astype('uint8')
+    batch_coords = []
+    batch_components = []
+    for ix, b in enumerate(binary_mask):
+        clusters, n_cands = lb(b)  # peforms connected component analysis.
+        uniques, counts = np.unique(clusters, return_counts=True)
+        # only keep n_components largest components.
+        keep_uniques = uniques[1:][np.argsort(counts[1:])[::-1]][:n_components]
+        # separate clusters and concat.
+        p_components = np.array([(clusters == ii) * 1 for ii in keep_uniques])
+        p_coords = []
+        if p_components.shape[0] > 0:
+            for roi in p_components:
+                mask_ixs = np.argwhere(roi != 0)
+
+                # get coordinates around component.
+                roi_coords = [np.min(mask_ixs[:, 0]) - 1, np.min(mask_ixs[:, 1]) - 1, np.max(mask_ixs[:, 0]) + 1,
+                              np.max(mask_ixs[:, 1]) + 1]
+                if dim == 3:
+                    roi_coords += [np.min(mask_ixs[:, 2]), np.max(mask_ixs[:, 2])+1]
+                p_coords.append(roi_coords)
+
+            p_coords = np.array(p_coords)
+
+            # clip coords.
+            p_coords[p_coords < 0] = 0
+            p_coords[:, :4][p_coords[:, :4] > binary_mask.shape[-2]] = binary_mask.shape[-2]
+            if dim == 3:
+                p_coords[:, 4:][p_coords[:, 4:] > binary_mask.shape[-1]] = binary_mask.shape[-1]
+
+        batch_coords.append(p_coords)
+        batch_components.append(p_components)
+    return batch_coords, batch_components
+
diff -ruN medicaldetectiontoolkit/predictor.py revised/predictor.py
--- medicaldetectiontoolkit/predictor.py	2021-06-28 22:59:37.734392830 +0100
+++ revised/predictor.py	2021-06-24 23:16:11.700027755 +0100
@@ -145,24 +145,33 @@
             self.rank_ix = str(rank_ix)  # get string of current rank for unique patch ids.
 
             with torch.no_grad():
-                for _ in range(batch_gen['n_test']):
+                print("batch_gen n_test %d" % batch_gen["n_test"])
+                for test_num in range(batch_gen['n_test']):
 
                     batch = next(batch_gen['test'])
 
                     # store batch info in patient entry of results dict.
                     if rank_ix == 0:
+                    # if batch["pid"] not in dict_of_patient_results:
                         dict_of_patient_results[batch['pid']] = {}
                         dict_of_patient_results[batch['pid']]['results_list'] = []
                         dict_of_patient_results[batch['pid']]['patient_bb_target'] = batch['patient_bb_target']
                         dict_of_patient_results[batch['pid']]['patient_roi_labels'] = batch['patient_roi_labels']
+                        dict_of_patient_results[batch['pid']]['seg_results_list'] = []
+                        dict_of_patient_results[batch['pid']]['inst_results_list'] = []
 
                     # call prediction pipeline and store results in dict.
                     results_dict = self.predict_patient(batch)
                     dict_of_patient_results[batch['pid']]['results_list'].append(results_dict['boxes'])
-
+                    dict_of_patient_results[batch['pid']]['seg_results_list'].append(results_dict['seg_preds'])
+                    dict_of_patient_results[batch['pid']]['inst_results_list'].append(results_dict['inst_preds'])
+                    print("recording pid %d seg shape %s" % (batch["pid"], results_dict['seg_preds'].shape))
+                    print("num boxes %d inst segs %d" % (len(results_dict["boxes"][0]), len(results_dict["inst_preds"])))
 
         self.logger.info('finished predicting test set. starting post-processing of predictions.')
         list_of_results_per_patient = []
+        results_per_patient_seg = {}
+        results_per_patient_inst = {}
 
         # loop over patients again to flatten results across epoch predictions.
         # if provided, add ground truth boxes for evaluation.
@@ -175,9 +184,21 @@
                                      for batch_instance in range(len(tmp_ens_list[0]))]
 
             # TODO return for instance segmentation:
+            tmp_ens_list_seg = p_dict['seg_results_list']
             # results_dict['seg_preds'] = np.mean(results_dict['seg_preds'], 1)[:, None]
-            # results_dict['seg_preds'] = np.array([[item for d in tmp_ens_list for item in d['seg_preds'][batch_instance]]
-            #                                       for batch_instance in range(len(tmp_ens_list[0]['boxes']))])
+
+            # tmp_ens_list = list len 1, tmp_ens_list[0] = list len 1 (?), tmp_ens_list[0][0] = list len 960 ?
+            results_dict['seg_preds'] = np.array([[item for d in tmp_ens_list_seg for item in d[batch_instance]]
+                                                 for batch_instance in range(len(tmp_ens_list_seg[0]))])
+
+            # assume first dimension are instances of segmentation (?)
+            # flatten second dimension assuming it is based on augmentations.
+            # results_dict['seg_preds'] = np.concatenate(tmp_ens_list_seg, axis=0).mean(axis=1, keepdims=True)
+
+            # results_dict["inst_preds"] = np.concatenate(p_dict['inst_results_list'], axis=0)
+            results_dict["inst_preds"] = p_dict['inst_results_list']
+
+            # print("pid %d seg_preds shape %s inst_preds shape %s" % (pid, results_dict["seg_preds"].shape, results_dict["inst_preds"].shape))
 
             # add 3D ground truth boxes for evaluation.
             for b in range(p_dict['patient_bb_target'].shape[0]):
@@ -187,34 +208,136 @@
                                                      'box_type': 'gt'})
 
             list_of_results_per_patient.append([results_dict['boxes'], pid])
+            if self.cf.segmentation_method == "global":
+                if pid not in results_per_patient_seg:
+                    results_per_patient_seg[pid] = []
+                results_per_patient_seg[pid].append(results_dict['seg_preds'])
+            elif self.cf.segmentation_method == "instance":
+                if pid not in results_per_patient_inst:
+                    results_per_patient_inst[pid] = []
+                results_per_patient_inst[pid].append(results_dict['inst_preds'])
 
         # save out raw predictions.
         out_string = 'raw_pred_boxes_hold_out_list' if self.cf.hold_out_test_set else 'raw_pred_boxes_list'
         with open(os.path.join(self.cf.fold_dir, '{}.pickle'.format(out_string)), 'wb') as handle:
             pickle.dump(list_of_results_per_patient, handle)
 
+        # save out raw segmentations.
+        # find mean of segmentation predictions (don't use WBC)
+        # for pid, segs in results_per_patient_seg.items():
+        #     # merged_segmentation = np.mean(segs, axis=0)
+        #     merged_segmentation = np.concatenate(segs, axis=0)
+        #
+        #     # print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #     print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #
+        #     out_file = os.path.join(self.cf.fold_dir, "seg_pid_%d.pickle" % pid)
+        #     with open(out_file, "wb") as fh:
+        #         pickle.dump(segs, fh)
+
+        # save out raw segmentations.
+        # find mean of segmentation predictions (don't use WBC)
+        # for pid, segs in results_per_patient_inst.items():
+        #     # merged_segmentation = np.mean(segs, axis=0)
+        #     merged_segmentation = np.concatenate(segs, axis=0)
+        #
+        #     # print("pid %d insts %d merged seg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #     print("pid %d insts %d merged instseg shape %s mean %f max %f min %f dtype %s" % (pid, len(segs), merged_segmentation.shape, merged_segmentation.mean(), merged_segmentation.max(), merged_segmentation.min(), merged_segmentation.dtype))
+        #
+        #     out_file = os.path.join(self.cf.fold_dir, "instseg_pid_%d.pickle" % pid)
+        #     with open(out_file, "wb") as fh:
+        #         pickle.dump(segs, fh)
+
         if return_results:
 
             # consolidate predictions.
             self.logger.info('applying wcs to test set predictions with iou = {} and n_ens = {}.'.format(
                 self.cf.wcs_iou, self.n_ens))
-            pool = Pool(processes=6)
-            mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.wcs_iou, self.n_ens] for ii in list_of_results_per_patient]
-            list_of_results_per_patient = pool.map(apply_wbc_to_patient, mp_inputs, chunksize=1)
-            pool.close()
-            pool.join()
+            run_parallel = False
+            mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.wcs_iou, self.n_ens] for ii in
+                         list_of_results_per_patient]
+            if run_parallel:
+                pool_processes = 6
+                pool = Pool(processes=pool_processes)
+                list_of_results_per_patient = pool.map(apply_wbc_to_patient, mp_inputs, chunksize=1)
+                pool.close()
+                pool.join()
+            else:
+                list_of_results_per_patient = [
+                    apply_wbc_to_patient(mp_input)
+                    for mp_input in mp_inputs
+                ]
 
             # merge 2D boxes to 3D cubes. (if model predicts 2D but evaluation is run in 3D)
             if self.cf.merge_2D_to_3D_preds:
                 self.logger.info('applying 2Dto3D merging to test set predictions with iou = {}.'.format(self.cf.merge_3D_iou))
-                pool = Pool(processes=6)
+                pool = Pool(processes=pool_processes)
                 mp_inputs = [[ii[0], ii[1], self.cf.class_dict, self.cf.merge_3D_iou] for ii in list_of_results_per_patient]
                 list_of_results_per_patient = pool.map(merge_2D_to_3D_preds_per_patient, mp_inputs, chunksize=1)
                 pool.close()
                 pool.join()
 
+            # combine instance segmentations based on associations in reduced boxes
+            #instsegs_per_patient = {}
+            for p_list in list_of_results_per_patient:
+                pid = p_list[1]
+                patient_boxes = [b for x in p_list[0] for b in x]
+
+                if self.cf.segmentation_method == "global":
+                    # combine segmentations from various ensemble models and instances
+                    # augmentation into a single segmentation array
+                    seg_arrays = [y for x in results_per_patient_seg[pid] for y in x]
+                    merged_segs = np.mean([np.mean(x, axis=0) for x in seg_arrays], axis=0)
+                    # merged_segs just has the spatial dimensions (x,y,z), stored format is expected
+                    # to include additional dimensions for example for instances, so add dimensions
+                    merged_segs = merged_segs[None, None, :, :, :]
+                    print("pid %d mean %f max %f min %f dtype %s" % (pid, merged_segs.mean(), merged_segs.max(), merged_segs.min(), merged_segs.dtype))
+                elif self.cf.segmentation_method == "instance":
+                    # flatten to get instance segmentation arrays
+                    inst_seg_arrays = [z for x in results_per_patient_inst[pid] for y in x for z in y]
+                    seg_array_sizes = [x.shape[0] for x in inst_seg_arrays]
+                    indices = np.cumsum([0] + seg_array_sizes)
+
+                    merged_segs = np.array([
+                        self.merge_segmentation(inst_seg_arrays, indices, this_box["box_assocs"])
+                        for this_box in patient_boxes
+                    ])
+                    print("pid %d insts %d merged insts %s mean %f max %f min %f dtype %s" % (pid, indices[-1], merged_segs.shape, merged_segs.mean(), merged_segs.max(), merged_segs.min(), merged_segs.dtype))
+
+                #instsegs_per_patient[pid] = merged_segs
+
+                # write out merged instance segmentations to file
+                if self.cf.segmentation_method is not None:
+
+                    out_file = os.path.join(self.cf.fold_dir, "instseg_pid_%d.pickle" % pid)
+                    with open(out_file, "wb") as fh:
+                        pickle.dump(merged_segs, fh)
+
             return list_of_results_per_patient
 
+    def merge_segmentation(self, inst_seg_arrays, array_indices, box_assocs):
+        """
+        Merge multiple instance segmentations into combined segmentations, based on associations between
+        boxes after performing weighted box clustering.
+        :param inst_seg_arrays: List containing a number of instance segmentation arrays
+        :param array_indices: Array of cumulative size of each array
+        :param box_assocs: List of associated input boxes and weights corresponding with a given output box
+        """
+        def get_input_seg(idx):
+            array_idx = np.where((idx >= array_indices[:-1]) & (idx < array_indices[1:]))[0][0]
+            array_inst_idx = idx - array_indices[array_idx]
+            return inst_seg_arrays[array_idx][array_inst_idx]
+
+        # get segments from indexes
+        weighted_segs = []
+        weight_sum = 0.
+        for input_idx, weight in box_assocs:
+            input_seg = get_input_seg(input_idx) # shape (x,y,z)
+            weighted_segs.append(input_seg * weight)
+            weight_sum += weight
+
+        merged_seg = np.sum(weighted_segs, axis=0) / weight_sum
+        return merged_seg
 
     def load_saved_predictions(self, apply_wbc=False):
         """
@@ -361,6 +484,11 @@
                                  for batch_instance in range(org_img_shape[0])]
         results_dict['seg_preds'] = np.array([[item for d in results_list for item in d['seg_preds'][batch_instance]]
                                               for batch_instance in range(org_img_shape[0])])
+        # todo: remove check and allow varying batch_instance size (?)
+        assert org_img_shape[0] == 1
+        # results_dict['inst_preds'] = np.concatenate([d["inst_preds"] for d in results_list], axis=0)
+        results_dict['inst_preds'] = [d["inst_preds"] for d in results_list]
+
         if self.mode == 'val':
             results_dict['monitor_values'] = results_list[0]['monitor_values']
 
@@ -394,11 +522,22 @@
             # counts patch instances per pixel-position.
             patch_overlap_map = np.zeros_like(out_seg_preds, dtype='uint8')
 
+            # produce map of instance segmentations, in space of original image
+            # find total number of instances (corresponding with boxes)
+            num_insts = np.sum([x.shape[0] for x in patches_dict["seg_preds"]])
+            instance_segs = np.zeros((num_insts,) + batch["original_img_shape"][2:], dtype=np.float16)
+
             #unmold segmentation outputs. loop over patches.
+            inst_count = 0
             for pix, pc in enumerate(patch_crops):
+                this_instance_segs = patches_dict['seg_preds'][pix]
+                merged_segment = np.mean(this_instance_segs, axis=0, keepdims=True) # (1,x,y,z)
                 if self.cf.dim == 3:
-                    out_seg_preds[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += patches_dict['seg_preds'][pix][None]
+                    out_seg_preds[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += merged_segment #patches_dict['seg_preds'][pix][None]
                     patch_overlap_map[:, :, pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += 1
+
+                    instance_segs[inst_count:inst_count+this_instance_segs.shape[0], pc[0]:pc[1], pc[2]:pc[3], pc[4]:pc[5]] += this_instance_segs
+                    inst_count += this_instance_segs.shape[0]
                 else:
                     out_seg_preds[pc[4]:pc[5], :, pc[0]:pc[1], pc[2]:pc[3], ] += patches_dict['seg_preds'][pix]
                     patch_overlap_map[pc[4]:pc[5], :, pc[0]:pc[1], pc[2]:pc[3], ] += 1
@@ -406,6 +545,7 @@
             # take mean in overlapping areas.
             out_seg_preds[patch_overlap_map > 0] /= patch_overlap_map[patch_overlap_map > 0]
             results_dict['seg_preds'] = out_seg_preds
+            results_dict["inst_preds"] = instance_segs
 
             # unmold box outputs. loop over patches.
             for pix, pc in enumerate(patch_crops):
@@ -497,7 +637,7 @@
             results_dict = {}
             # flatten out batch elements from chunks ([chunk, chunk] -> [b, b, b, b, ...])
             results_dict['boxes'] = [item for d in chunk_dicts for item in d['boxes']]
-            results_dict['seg_preds'] = np.array([item for d in chunk_dicts for item in d['seg_preds']])
+            results_dict['seg_preds'] = [item for d in chunk_dicts for item in d['seg_preds']]
 
             if self.mode == 'val':
                 # estimate metrics by mean over batch_chunks. Most similar to training metrics.
@@ -536,13 +676,14 @@
             box_patch_id = np.array([b[1]['patch_id'] for b in boxes])
 
             if 0 not in box_scores.shape:
-                keep_scores, keep_coords = weighted_box_clustering(
+                keep_scores, keep_coords, keep_assocs = weighted_box_clustering(
                     np.concatenate((box_coords, box_scores[:, None], box_center_factor[:, None],
                                     box_n_overlaps[:, None]), axis=1), box_patch_id, wcs_iou, n_ens)
 
                 for boxix in range(len(keep_scores)):
                     out_patient_results_list[bix].append({'box_type': 'det', 'box_coords': keep_coords[boxix],
-                                             'box_score': keep_scores[boxix], 'box_pred_class_id': cl})
+                                             'box_score': keep_scores[boxix], 'box_pred_class_id': cl,
+                                                          'box_assocs': keep_assocs[boxix]})
 
         # add gt boxes back to new output list.
         out_patient_results_list[bix].extend([box for box in b if box['box_type'] == 'gt'])
@@ -608,6 +749,7 @@
     :param n_ens: number of models, that are ensembled. (-> number of expected predicitions per position)
     :return: keep_scores: (n_keep)  new scores of boxes to be kept.
     :return: keep_coords: (n_keep, (y1, x1, y2, x2, (z1), (z2)) new coordinates of boxes to be kept.
+    :return: keep_assocs: (n_keep, [input_idx, score]) list of associated input boxes and scores for each returned box
     """
     dim = 2 if dets.shape[1] == 7 else 3
     y1 = dets[:, 0]
@@ -631,6 +773,7 @@
     keep = []
     keep_scores = []
     keep_coords = []
+    keep_assocs = []
 
     while order.size > 0:
         i = order[0]  # higehst scoring element
@@ -698,12 +841,13 @@
         if avg_score > 0.01:
             keep_scores.append(avg_score)
             keep_coords.append(avg_coords)
+            keep_assocs.append(list(zip(matches, match_scores)))
 
         # get index of all elements that were not matched and discard all others.
         inds = np.where(ovr <= thresh)[0]
         order = order[inds]
 
-    return keep_scores, keep_coords
+    return keep_scores, keep_coords, keep_assocs
 
 
 
diff -ruN medicaldetectiontoolkit/README.md revised/README.md
--- medicaldetectiontoolkit/README.md	2021-06-28 22:59:37.707392831 +0100
+++ revised/README.md	2020-10-18 20:11:10.636575637 +0100
@@ -21,7 +21,7 @@
 <br/>
 [1] He, Kaiming, et al.  <a href="https://arxiv.org/abs/1703.06870">"Mask R-CNN"</a> ICCV, 2017<br>
 [2] Lin, Tsung-Yi, et al.  <a href="https://arxiv.org/abs/1708.02002">"Focal Loss for Dense Object Detection"</a> TPAMI, 2018.<br>
-[3] Jaeger, Paul et al. <a href="https://ml4health.github.io/2019/pdf/232_ml4h_preprint.pdf"> "Retina U-Net: Embarrassingly Simple Exploitation
+[3] Jaeger, Paul et al. <a href="http://arxiv.org/abs/1811.08661"> "Retina U-Net: Embarrassingly Simple Exploitation
 of Segmentation Supervision for Medical Object Detection" </a>, 2018
 
 [5] https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py<br/>
diff -ruN medicaldetectiontoolkit/requirements.txt revised/requirements.txt
--- medicaldetectiontoolkit/requirements.txt	2021-06-28 22:59:37.734392830 +0100
+++ revised/requirements.txt	2020-10-18 20:25:32.259622834 +0100
@@ -10,7 +10,7 @@
 networkx==2.4
 numpy==1.15.3
 pandas==0.23.4
-Pillow==8.1.1
+Pillow==7.1.0
 pycparser==2.19
 pyparsing==2.4.5
 python-dateutil==2.8.1
@@ -22,6 +22,6 @@
 six==1.13.0
 sklearn==0.0
 threadpoolctl==1.1.0
-torch==0.4.1
+#torch==0.4.1
 traceback2==1.4.0
 unittest2==1.1.0
